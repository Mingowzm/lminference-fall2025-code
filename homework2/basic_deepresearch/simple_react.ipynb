{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import litellm\n",
    "import random\n",
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mcp_agents.tool_interface.base import *\n",
    "from mcp_agents.tool_interface.mcp_tools import *\n",
    "from mcp_agents.client import *\n",
    "from mcp_agents.agent_interface import *\n",
    "from mcp_agents.evaluation_utils.utils import *\n",
    "\n",
    "# !playwright install #to run the crawl4ai tool\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<your_openai_api_key>\"\n",
    "os.environ[\"SERPER_API_KEY\"] = \"<your_serper_api_key>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7fca3",
   "metadata": {},
   "source": [
    "### Build a search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from mcp_agents.tool_interface.mcp_tools\n",
    "\n",
    "search_tool = SerperSearchTool(\n",
    "    tool_start_tag=\"<query>\",\n",
    "    tool_end_tag=\"</query>\",\n",
    "    result_start_tag=\"<snippet>\",\n",
    "    result_end_tag=\"</snippet>\",\n",
    "    number_documents_to_search=2,\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "client = LLMToolClient(\n",
    "    model_name=\"openai/gpt-4o\",  # Dummy model name\n",
    "    tokenizer_name=\"openai/gpt-4o\",  # Dummy model name\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    tools=[search_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6dee221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 11-664/763: Inference Algorithms for Language Modeling\n",
      "URL: http://phontron.com/class/lminference-fall2025/\n",
      "Snippet: Inference-time algorithms can be applied on top of an already-trained model to improve generation quality, lower latency, or induce additional controllability.\n",
      "\n",
      "Title: Introduction to Language Models and Inference - YouTube\n",
      "URL: https://www.youtube.com/watch?v=F-mduXzNcRQ\n",
      "Snippet: This lecture (by Graham Neubig) for CMU CS 11-763, Advanced NLP (Fall 2025) covers: What is a language model? What is an inference algorithm ...\n"
     ]
    }
   ],
   "source": [
    "output = await search_tool(\"<query>Advisors offering Inference Algorithms for Language Modeling classes</query>\")\n",
    "print(search_tool.format_result(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67b235",
   "metadata": {},
   "source": [
    "### A basic ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'react_agent_base_url': 'https://api.openai.com/v1', 'react_agent_model_name': 'openai/gpt-4o', 'react_agent_tokenizer_name': 'openai/gpt-4o', 'react_agent_api_key': '<your_openai_api_key>', 'search_tool_name': 'serper', 'number_documents_to_search': 2, 'search_timeout': 60, 'browse_tool_name': 'crawl4ai', 'browse_timeout': 60, 'browse_max_pages_to_fetch': 2, 'num_think_search_cycles': 3}\n"
     ]
    }
   ],
   "source": [
    "from react_agent import *\n",
    "\n",
    "default_config_path = \"./react_agent.yaml\"\n",
    "\n",
    "workflow = ReActWorkflow(configuration=default_config_path)\n",
    "\n",
    "# print the config\n",
    "\n",
    "output = await workflow(\n",
    "    # question=\"Who are the target audience for CMU 11-763 Inference Algorithms for Language Modeling classes?\",\n",
    "    question=\"Who are the staff members for CMU 11-763 Inference Algorithms for Language Modeling classes?\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "final_answer, results, conversation_history, searched_queries = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21426178",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer, results, conversation_history, searched_queries = output\n",
    "print(\"Model answer:\")\n",
    "print(final_answer)\n",
    "print(\"----------------------\")\n",
    "print(\"Tool calls:\")\n",
    "print(dict(results)[\"tool_calls\"][1][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7713ce",
   "metadata": {},
   "source": [
    "### Your Task: the pipeline for Short-form Tasks\n",
    "\n",
    "You will work on applying the agent you just built to the graph and MMLU problems you explored in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Example 1: Perfect prediction\n",
      "============================================================\n",
      "dict_keys(['score', 'matches', 'expected', 'predicted_count', 'correct_paths_found', 'incorrect_paths', 'missing_paths', 'message'])\n"
     ]
    }
   ],
   "source": [
    "# Build a simple agent for the graph problem\n",
    "from graph.graph_path_finder import *\n",
    "# YOUR_TASK_2.1, fix the GraphPathEvaluationTool (3 tasks, 6 lines of code)\n",
    "from mcp_agents.tool_interface.mcp_tools import GraphPathEvaluationTool\n",
    "\n",
    "correct_paths = [\n",
    "    {\"path\": [0, 1, 3, 7], \"weight\": 25},\n",
    "    {\"path\": [0, 2, 5, 7], \"weight\": 30}\n",
    "]\n",
    "\n",
    "eval_tool = GraphPathEvaluationTool(\n",
    "    correct_paths=correct_paths,\n",
    "    expected_count=2,\n",
    "    tool_start_tag=\"<predicted_paths>\",\n",
    "    tool_end_tag=\"</predicted_paths>\",\n",
    "    result_start_tag=\"<evaluation>\",\n",
    "    result_end_tag=\"</evaluation>\",\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(\"=== Correct Prediction ===\\n\")\n",
    "\n",
    "input1 = \"\"\"<predicted_paths>\n",
    "{\n",
    "    \"paths\": [[0, 1, 3, 7], [0, 2, 5, 7]],\n",
    "    \"weights\": [25, 30]\n",
    "}\n",
    "</predicted_paths>\"\"\"\n",
    "\n",
    "output1 = await eval_tool(input1)\n",
    "print(output1.keys())\n",
    "# print(json.dumps(output1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "885bd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph Path Finding Example ===\n",
      "\n",
      "1. Creating a random graph...\n",
      "Graph parameters: N=5, M=2, W=50, P=1\n",
      "Edges:\n",
      "  0 -> 1 (weight: 20)\n",
      "  0 -> 2 (weight: 25)\n",
      "  1 -> 2 (weight: 49)\n",
      "  1 -> 3 (weight: 19)\n",
      "  2 -> 3 (weight: 19)\n",
      "  2 -> 4 (weight: 17)\n",
      "  3 -> 4 (weight: 26)\n",
      "  3 -> 0 (weight: 11)\n",
      "  4 -> 0 (weight: 9)\n",
      "  4 -> 1 (weight: 44)\n",
      "\n",
      "2. Finding shortest path with dynamic programming...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Graph Path Finding Example ===\\n\")\n",
    "\n",
    "# Create a simple example graph\n",
    "print(\"1. Creating a random graph...\")\n",
    "edges, params = create_random_graph(N=5, M=2, W=50, P=1)\n",
    "\n",
    "print(f\"Graph parameters: N={params['N']}, M={params['M']}, W={params['W']}, P={params['P']}\")\n",
    "print(\"Edges:\")\n",
    "for src, dst, weight in edges:\n",
    "    print(f\"  {src} -> {dst} (weight: {weight})\")\n",
    "\n",
    "# Find the correct solution\n",
    "print(\"\\n2. Finding shortest path with dynamic programming...\")\n",
    "solution = find_top_p_paths(edges, params[\"N\"], params[\"P\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0465c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_problem_prompt(edges, params[\"N\"], params[\"P\"])\n",
    "\n",
    "llm_response = query_llm_with_function_call(prompt, \"gpt-4o\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "predicted_solution = convert_llm_response_to_solution(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "315e890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Prediction ===\n",
      "\n",
      "dict_keys(['score', 'matches', 'expected', 'predicted_count', 'correct_paths_found', 'incorrect_paths', 'missing_paths', 'message'])\n",
      "{\n",
      "  \"score\": 1.0,\n",
      "  \"matches\": 1,\n",
      "  \"expected\": 1,\n",
      "  \"predicted_count\": 1,\n",
      "  \"correct_paths_found\": [\n",
      "    {\n",
      "      \"path\": [\n",
      "        0,\n",
      "        2,\n",
      "        4\n",
      "      ],\n",
      "      \"weight\": 42\n",
      "    }\n",
      "  ],\n",
      "  \"incorrect_paths\": [],\n",
      "  \"missing_paths\": [],\n",
      "  \"message\": \"Found 1/1 correct paths (100.0%)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def solution_to_dict_list(solution: GraphPathSolution) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert GraphPathSolution to list of dict format.\n",
    "    \n",
    "    Args:\n",
    "        solution: GraphPathSolution object\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'path' and 'weight' keys\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"path\": path_info.path, \"weight\": path_info.weight}\n",
    "        for path_info in solution.paths\n",
    "    ]\n",
    "\n",
    "\n",
    "eval_tool = GraphPathEvaluationTool(\n",
    "    correct_paths=solution_to_dict_list(solution),\n",
    "    expected_count=len(solution_to_dict_list(solution)),\n",
    "    tool_start_tag=\"<predicted_paths>\",\n",
    "    tool_end_tag=\"</predicted_paths>\",\n",
    "    result_start_tag=\"<evaluation>\",\n",
    "    result_end_tag=\"</evaluation>\",\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(\"=== Model Prediction ===\\n\")\n",
    "\n",
    "input2 = f\"\"\"<predicted_paths>\n",
    "{json.dumps(solution_to_dict_list(predicted_solution), indent=2)}\n",
    "</predicted_paths>\"\"\"\n",
    "\n",
    "output2 = await eval_tool(input2)\n",
    "print(output2.keys())\n",
    "print(json.dumps(output2, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e0b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 173 examples\n"
     ]
    }
   ],
   "source": [
    "# Build the inference pipeline for MMLU\n",
    "from inference.inference import load_custom_dataset, convert_llm_response_to_solution, format_example, format_subject\n",
    "\n",
    "examples = load_custom_dataset(\"MMLU-preview\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate generation and evaluation functions\n",
    "async def udated_mmlu_pipeline(examples, default_config_path):\n",
    "    \"\"\"Generate responses for all examples without evaluation\"\"\"\n",
    "    \n",
    "    print(\"Generating responses for\", len(examples), \"MMLU examples\")\n",
    "\n",
    "    workflow = ReActWorkflow(configuration=default_config_path)\n",
    "    base_agent_prompt = workflow.answer_agent.prompt\n",
    "\n",
    "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "    results = []\n",
    "    total_score = 0.0\n",
    "\n",
    "    for i, example in tqdm(enumerate(examples, 1), total=len(examples), desc=\"Evaluating examples\"):\n",
    "        question = example[\"question\"] # format_example(example, include_answer=False)\n",
    "        correct_answer = choices[example[\"answer\"]]\n",
    "\n",
    "        # YOUR_TASK_2.2: what is the additional instructions here?\n",
    "        # Hint: check the MMLU inference pipeline to understand what to specify; 1 line of code\n",
    "\n",
    "        workflow.answer_agent.prompt = base_agent_prompt + \"\\n\" + additional_instructions + format_example(example, include_answer=False)\n",
    "\n",
    "        output = await workflow(\n",
    "            question=question,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.7,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        final_answer, answer_result, conversation_history, searched_queries = output\n",
    "\n",
    "        predicted_solution = convert_llm_response_to_solution(final_answer, \"MMLU\")\n",
    "\n",
    "        score = (choices[example[\"answer\"]] == predicted_solution)\n",
    "        total_score += score\n",
    "\n",
    "        results.append({\n",
    "            \"example_id\": i,    \n",
    "            \"question\": question,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_solution\": predicted_solution,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"generation\": answer_result.model_dump(),\n",
    "            \"conversation_history\": conversation_history,\n",
    "            \"searched_queries\": searched_queries,\n",
    "        })\n",
    "\n",
    "    average_score = total_score / len(examples) if examples else 0.0\n",
    "\n",
    "    print(f\"Average score: {average_score:.2f}\")\n",
    "    \n",
    "    output_config = {k:v for k,v in dict(workflow.configuration).items() if \"api_key\" not in k}\n",
    "\n",
    "    return {\n",
    "        \"config\": output_config,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_examples\": len(examples),\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "\n",
    "default_config_path = \"./react_agent_mmlu.yaml\"\n",
    "\n",
    "# YOUR_TASK_2.2: Run the inference for 30 examples after fixing the function:\n",
    "# save the answers using the code at the next block\n",
    "# report the acc. at the home write-up\n",
    "output = await udated_mmlu_pipeline(examples[:30], default_config_path)\n",
    "\n",
    "# Inspect or save generated responses here if needed\n",
    "print(f\"Generated {len(output['results'])} responses\")\n",
    "print(\"Sample response:\")\n",
    "print(output[\"results\"][0][\"final_answer\"])\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86191ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "0.85\n",
      "## Question: A police officer carries out hundreds of traffic stops every year. When his supervisor is reviewing the officer’s records for the past year, he notices that the officer is equally likely to stop people of various genders, ages, and races. However, he is significantly more likely to write tickets for middle-aged white males with dark hair and eyes. When confronted with this fact, the officer truthfully states that he has no idea why that is, and that it must simply be a coincidence. Unbeknownst to the officer, this behavior is tied to the fact that these men look like his father, with whom he had an abusive relationship as a child. What psychological framework would directly address the unconscious bias in his behavior? \n",
      "## Correct Answer: B\n",
      "## Final Answer:\n",
      "The situation described involves the police officer exhibiting unconscious bias in his decision to write tickets, which is influenced by his childhood experiences, particularly the abusive relationship with his father. This behavior can be understood through the lens of psychological frameworks that delve into unconscious motivations, emotional responses, and the impact of early experiences on adult behavior.\n",
      "\n",
      "1. **Implicit Bias Theory**: This theory suggests that individuals hold automatic associations between certain groups and stereotypes, which can influence their behaviors and decision-making without conscious awareness. The officer's tendency to ticket a specific demographic aligns with implicit bias, which is critical to understanding his actions.\n",
      "\n",
      "2. **Psychological Projection**: This concept refers to the unconscious defense mechanism where individuals attribute their own unacceptable feelings or thoughts onto others. In this case, the officer may be projecting his unresolved feelings about his father onto individuals who resemble him, leading to biased enforcement actions.\n",
      "\n",
      "3. **Impact of Childhood Trauma**: Childhood experiences, particularly trauma, can significantly shape an individual's perceptions and behaviors in adulthood. The officer's abusive relationship with his father likely contributes to his unconscious biases, influencing how he interacts with and perceives specific groups.\n",
      "\n",
      "Considering these factors, the psychological framework that most directly addresses the unconscious bias displayed by the officer is the **Psychoanalytic** approach. This framework emphasizes the influence of unconscious processes and childhood experiences on behavior, which is highly relevant in this context.\n",
      "\n",
      "Thus, the answer is B. Psychoanalytic.\n",
      "## Searched Queries: ['implicit bias theory in law enforcement childhood trauma influence psychological projection interventions', '']\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "## Question: Who set the world record for the mile race in 1886?\n",
      "## Correct Answer: D\n",
      "## Final Answer:\n",
      "The world record for the mile race in 1886 was set by Walter George. He achieved this remarkable feat on August 23, 1886, at Lillie Bridge in London, with a time of 4:12.8 (often noted as 4:12¾ at the time). This record was significant as it stood unbroken for nearly 30 years, highlighting George's exceptional talent in middle-distance running. The race attracted a large crowd of approximately 20,000 spectators, creating an electric atmosphere for this historic event.\n",
      "\n",
      "George's achievement not only marked a personal triumph but also had a lasting impact on the sport of athletics, setting a benchmark for future middle-distance runners. The record was particularly notable as it was the first sub-4:20 mile, a significant milestone in the history of running.\n",
      "\n",
      "The answer is (D).\n",
      "## Searched Queries: ['world record mile race 1886 holder athlete time location', 'Walter George mile race world record 1886 details time conditions significance']\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = output[\"config\"][\"react_agent_model_name\"]\n",
    "model_display = model.split(\"/\")[-1]\n",
    "len_examples = output[\"total_examples\"]\n",
    "display_config = default_config_path.split(\"/\")[-1].replace(\".yaml\", \"\")\n",
    "print(model_display)\n",
    "\n",
    "# with open(f\"results_{model_display}_{len_examples}_{display_config}.json\", \"w\") as f:\n",
    "#     json.dump(results, f, indent=2)\n",
    "\n",
    "print(output[\"average_score\"])\n",
    "\n",
    "for response in output[\"results\"][:2]:\n",
    "    print(\"## Question: \" + response[\"question\"])\n",
    "    print(\"## Correct Answer: \" + response[\"correct_answer\"])\n",
    "    print(\"## Final Answer:\\n\" + response[\"final_answer\"])\n",
    "    print(\"## Searched Queries: \" + str(response[\"searched_queries\"]))\n",
    "    print(\"----\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4ccd7",
   "metadata": {},
   "source": [
    "### Simple analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3adc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 93.95,\n",
       " 'thinking': 684.3,\n",
       " 'query': 27.25,\n",
       " 'snippets_titles': 141.1,\n",
       " 'snippets_snippets': 1072.45,\n",
       " 'final_answer': 270.5}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_thoughts(text: str) -> str:\n",
    "    match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "def extract_search_query(text: str) -> str:\n",
    "    match = re.search(r\"<query>(.*?)</query>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "\n",
    "def parse_xml_snippets(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"<snippet id=([^>]+)>\"  # Capture Group 1: The ID\n",
    "        # YOUR_TASK_3.1: three lines of code here to process the retrieved snippets\n",
    "        #<your 1st line of code>         # Capture Group 2: The Title\n",
    "        #<your 2nd line of code>      # Optional Group with Capture Group 3: The URL\n",
    "        #<your 3rd line of code>       # Capture Group 4: The Snippet\n",
    "        r\"\\s*</snippet>\",          # The closing tag\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        # match is a tuple: (id, title, url, snippet)\n",
    "        # If the optional URL group did not match, match[2] will be None.\n",
    "        snippet_id = match[0].strip()\n",
    "        title = match[1].strip()\n",
    "        url = match[2].strip() if match[2] else \"\"  # Handle None case for missing URL\n",
    "        snippet = match[3].strip()\n",
    "\n",
    "        results.append({\n",
    "            \"Title\": title,\n",
    "            \"URL\": url,\n",
    "            \"Snippet\": snippet\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def count_tokens(text, model=\"openai/gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a prompt using LiteLLM's token counting utility.\n",
    "    Args:\n",
    "        prompt (str): The input prompt string.\n",
    "        model (str): The model name for which to count tokens (default: \"gpt-3.5-turbo\").\n",
    "    Returns:\n",
    "        int: The number of tokens in the prompt.\n",
    "    \"\"\"\n",
    "    return litellm.token_counter(model=model, messages=[{\"role\": \"user\", \"content\": text}])\n",
    "\n",
    "\n",
    "def count_tokens_in_results(results):\n",
    "    # report the numbers of tokens used for question,thinking, query, snippets, and final answer\n",
    "    report_results = []\n",
    "    for result in results:\n",
    "\n",
    "        thoughts = [one_round[\"content\"] for one_round in result[\"conversation_history\"] if one_round[\"type\"] == \"think\"]\n",
    "        cleaned_thoughts = [extract_thoughts(thought) for thought in thoughts]\n",
    "        \n",
    "        query_snippets = [one_round[\"content\"] for one_round in result[\"conversation_history\"] if one_round[\"type\"] == \"query\"]\n",
    "        cleaned_queries = [extract_search_query(query) for query in query_snippets]\n",
    "        # print(parse_xml_snippets(query_snippets[0]))\n",
    "        parsed_snippets = []\n",
    "        for query in query_snippets:\n",
    "            parsed_snippets.extend(parse_xml_snippets(query))\n",
    "        cleaned_snippets_titles = [snippet[\"Title\"] for snippet in parsed_snippets]\n",
    "        cleaned_snippets_snippets = [snippet[\"Snippet\"] for snippet in parsed_snippets]\n",
    "\n",
    "        report_results.append({\n",
    "            \"question\": count_tokens(result[\"question\"]),\n",
    "            \"thinking\": count_tokens(\" \".join(cleaned_thoughts)),\n",
    "            \"query\": count_tokens(\" \".join(cleaned_queries)),   \n",
    "            \"snippets_titles\": count_tokens(\" \".join(cleaned_snippets_titles)),\n",
    "            \"snippets_snippets\": count_tokens(\" \".join(cleaned_snippets_snippets)),\n",
    "            \"final_answer\": count_tokens(result[\"final_answer\"]),\n",
    "        })\n",
    "\n",
    "    # for each key, report the average, round to 2 decimal places\n",
    "    return {key: round(sum(result[key] for result in report_results) / len(report_results), 2) for key in report_results[0].keys()}\n",
    "\n",
    "# YOUR_TASK_3.2: calculate the token counts for each category for each variant and report in the homework write-up\n",
    "count_tokens_in_results(output[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3a121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

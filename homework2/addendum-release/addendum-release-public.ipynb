{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6702de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with open(\"results/all_results_processed.json\") as f:\n",
    "    all_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c048553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'Qwen/Qwen3-4B',\n",
       " 'eval_model': 'gpt-5-nano-2025-08-07',\n",
       " 'sampling_params': {'n': 50, 'temperature': 0.2, 'max_tokens': 512},\n",
       " 'total_questions': 100,\n",
       " 'total_candidates': 5000,\n",
       " 'dataset': 'infobench'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5dcd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a78296c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_id', 'prompt', 'question_data', 'candidates'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['results'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e8c94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_results['results'][0]['candidates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bdb3a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['candidate_id', 'generated_text', 'finish_reason', 'token_ids', 'generation_len', 'full_chat', 'scores'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['results'][0]['candidates'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d01f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Below is a Python code snippet that constructs a two-hidden layer feedforward neural network using PyTorch\\'s `torch.nn` module. The network includes an input layer, two hidden layers with ReLU activation, and an output layer. The number of neurons in the hidden layers is chosen between 32 and 128 to maintain a reasonably sized network. Comments are included to clarify each step.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\n\\n# Define the neural network class\\nclass TwoHiddenLayerNetwork(nn.Module):\\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\\n        super(TwoHiddenLayerNetwork, self).__init__()\\n        \\n        # Input layer: input_size -> hidden_size1\\n        self.layer1 = nn.Linear(input_size, hidden_size1)\\n        \\n        # Hidden layer 1: hidden_size1 -> hidden_size2\\n        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\\n        \\n        # Hidden layer 2: hidden_size2 -> output_size\\n        self.layer3 = nn.Linear(hidden_size2, output_size)\\n        \\n        # ReLU activation function\\n        self.relu = nn.ReLU()\\n    \\n    def forward(self, x):\\n        # Forward pass through the network\\n        # Input to first hidden layer\\n        x = self.relu(self.layer1(x))\\n        # First hidden layer to second hidden layer\\n        x = self.relu(self.layer2(x))\\n        # Second hidden layer to output layer\\n        x = self.layer3(x)\\n        \\n        return x\\n\\n# Example usage\\nif __name__ == \"__main__\":\\n    # Define input and output sizes\\n    input_size = 10  # Example: 10 input features\\n    output_size = 1  # Example: 1 output neuron\\n    \\n    # Choose hidden layer sizes between 32 and 128\\n    hidden_size1 = 64\\n    hidden_size2 = 128\\n    \\n    # Instantiate the network\\n    model = TwoHiddenLayerNetwork(input_size, hidden_size1, hidden_size2, output_size)\\n    \\n    # Example input tensor (batch size 1, input size 10)\\n    input_tensor = torch.randn(1, input_size)\\n    \\n    # Forward pass\\n    output = model(input_tensor)\\n    print(\"Output:\", output)\\n```\\n\\n### Explanation of the Code:\\n\\n- **`TwoHiddenLayerNetwork` class**: This is a subclass of `nn.Module`,'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TIP: use these \"generated_text\" fields to count unique generations for each infobench question\n",
    "all_results['results'][0]['candidates'][0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85370f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 56.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# TIP: Use half precision when loading the model for efficiency. Providing one leading example\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16).to('cuda:0')\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c6f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['candidate_id', 'generated_text', 'finish_reason', 'token_ids', 'generation_len', 'full_chat', 'scores'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['results'][42]['candidates'][0].keys() \n",
    "# TIP: Remember that each of the 100 questions has 50 candidates\n",
    "# You will put your calculated scores in the 'scores' dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3538d43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor each in all_results[\\'results\\']:\\n    og_input = each[\\'question_data\\'][\\'input\\']\\n    og_instruction = each[\\'question_data\\'][\\'instruction\\']\\n    intermediate_prompt = each[\\'prompt\\']\\n    chat_form = [{\"role\": \"user\", \"content\": intermediate_prompt}]\\n    processed_prompt = tokenizer.apply_chat_template(chat_form, tokenize=False, add_generation_prompt=True, enable_thinking=False)\\n    prompt_tokens = tokenizer(processed_prompt)[\\'input_ids\\']\\n\\n    d = {\\'og_input\\': og_input,\\n         \\'og_instruction\\': og_instruction,\\n         \\'intermediate_prompt\\': intermediate_prompt,\\n         \\'chat_form\\': chat_form,\\n         \\'processed_prompt\\': processed_prompt,\\n         \\'prompt_tokens\\': prompt_tokens}\\n    \\n    each[\\'prompt\\'] = d\\n\\n    for candidate in each[\\'candidates\\']:\\n       full_chat_form = [{\"role\": \"user\", \"content\": intermediate_prompt}, {\"role\": \"assistant\", \"content\": candidate[\\'generated_text\\']}]\\n       full_chat_templated = tokenizer.apply_chat_template(full_chat_form, tokenize=False, add_generation_prompt=False)\\n       full_chat_tokens = tokenizer.apply_chat_template(full_chat_form)\\n       dd = {\\n           \\'full_chat_form\\': full_chat_form,\\n           \\'processed_full_chat\\': full_chat_templated,\\n           \\'full_chat_tokens\\': full_chat_tokens, \\n       }\\n       candidate[\\'full_chat\\'] = dd\\n       candidate[\\'scores\\'] = {\\'infobench\\': candidate.pop(\\'infobench_score\\'), # (just bringing this down into the scores dict)\\n                              \\'qwen3_4b\\': None,\\n                              \\'qwen3_13b\\': None,\\n                              \\'r_scalar\\': None,\\n                              \\'r_pairwise\\': None,\\n                              \\'mbr_bleu\\': None,\\n                              \\'mbr_bert\\': None,\\n                              }\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TIP: We are providing a strict superset of the information you need to complete this problem. \n",
    "# The json contains multiple different forms for each example and candidate. Here is the code we \n",
    "# originally used to make these forms.\n",
    "\n",
    "\"\"\"\n",
    "for each in all_results['results']:\n",
    "    og_input = each['question_data']['input']\n",
    "    og_instruction = each['question_data']['instruction']\n",
    "    intermediate_prompt = each['prompt']\n",
    "    chat_form = [{\"role\": \"user\", \"content\": intermediate_prompt}]\n",
    "    processed_prompt = tokenizer.apply_chat_template(chat_form, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "    prompt_tokens = tokenizer(processed_prompt)['input_ids']\n",
    "\n",
    "    d = {'og_input': og_input,\n",
    "         'og_instruction': og_instruction,\n",
    "         'intermediate_prompt': intermediate_prompt,\n",
    "         'chat_form': chat_form,\n",
    "         'processed_prompt': processed_prompt,\n",
    "         'prompt_tokens': prompt_tokens}\n",
    "    \n",
    "    each['prompt'] = d\n",
    "\n",
    "    for candidate in each['candidates']:\n",
    "       full_chat_form = [{\"role\": \"user\", \"content\": intermediate_prompt}, {\"role\": \"assistant\", \"content\": candidate['generated_text']}]\n",
    "       full_chat_templated = tokenizer.apply_chat_template(full_chat_form, tokenize=False, add_generation_prompt=False)\n",
    "       full_chat_tokens = tokenizer.apply_chat_template(full_chat_form)\n",
    "       dd = {\n",
    "           'full_chat_form': full_chat_form,\n",
    "           'processed_full_chat': full_chat_templated,\n",
    "           'full_chat_tokens': full_chat_tokens, \n",
    "       }\n",
    "       candidate['full_chat'] = dd\n",
    "       candidate['scores'] = {'infobench': candidate.pop('infobench_score'), # (just bringing this down into the scores dict)\n",
    "                              'qwen3_4b': None,\n",
    "                              'qwen3_13b': None,\n",
    "                              'r_scalar': None,\n",
    "                              'r_pairwise': None,\n",
    "                              'mbr_bleu': None,\n",
    "                              'mbr_bert': None,\n",
    "                              }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3915df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'infobench': 1.0,\n",
       " 'qwen3_4b': None,\n",
       " 'qwen3_13b': None,\n",
       " 'r_scalar': None,\n",
       " 'r_pairwise': None,\n",
       " 'mbr_bleu': None,\n",
       " 'mbr_bert': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['results'][0]['candidates'][20]['scores']\n",
    "# TIP: this is where you'll want to fill in the different scores you calculate\n",
    "# \"oracle\" infobench score has been provided for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a63456",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_toks = all_results['results'][0]['prompt']['prompt_tokens']\n",
    "\n",
    "out_toks = all_results['results'][0]['candidates'][20]['token_ids']\n",
    "\n",
    "all_toks = all_results['results'][0]['candidates'][20]['full_chat']['full_chat_tokens']\n",
    "\n",
    "model_input = torch.tensor([all_toks]).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b00079",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(model_input).logits\n",
    "    log_probs = F.log_softmax(logits[0], dim=-1)\n",
    "\n",
    "# TIP: we're giving you this much with the above example because the harder part is yet to come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b6e4859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 623]), 109, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape, len(in_toks), len(out_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIP: calculate a logprob score with by accumulating a sum over the relevant \n",
    "# logits... For the exact example above, the value should be greater than -30\n",
    "# (as in, less negative than -30)\n",
    "cumulative_logprobs = 0\n",
    "\n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67712009",
   "metadata": {},
   "source": [
    "... and now you have the rest of the problem to do :) Just remember to put your scores in\n",
    "the `scores` dict for each candidate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf-algs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

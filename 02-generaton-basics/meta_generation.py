# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.4
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# # Simplified Meta-Generation: Cross-Model Likelihood Analysis
#
# By Graham Neubig for [11-664/763 Inference Algorithms for Language Modeling](https://phontron.com/class/lminference-fall2025/)
#
# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neubig/lminference-fall2025-site/blob/main/_slides/2025-09-02-sampling-methods/meta_generation_experiments.ipynb)
#
# This simplified notebook explores how different language models evaluate text generated by other models.
# We generate 100 outputs from GPT-2 small and evaluate them using both small and medium models.

# %%
from __future__ import annotations

from dataclasses import dataclass
from typing import List

import torch
import torch.nn.functional as F
from nanogpt import GPT2, GPT2Config, GPT2Tokenizer
from generation import generate_with_temperature, temperature_sample


@dataclass
class GenerationResult:
    """Result of a single generation with cross-model evaluation"""
    prompt: str
    generated_text: str
    small_log_prob: float
    medium_log_prob: float
    small_per_token_log_prob: float
    medium_per_token_log_prob: float
    token_count: int


def load_models(device: str = "auto") -> tuple[GPT2, GPT2, GPT2Tokenizer]:
    """Load small and medium GPT-2 models"""
    if device == "auto":
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
    
    print(f"Loading models on {device}...")
    
    # Create tokenizer
    tokenizer = GPT2Tokenizer()
    
    # Load small model
    small_config = GPT2Config(n_layer=6, n_head=6, n_embd=384)
    small_model = GPT2(small_config)
    small_model.to(device)
    small_model.eval()
    
    # Load medium model
    medium_config = GPT2Config(n_layer=12, n_head=12, n_embd=768)
    medium_model = GPT2(medium_config)
    medium_model.to(device)
    medium_model.eval()
    
    print("Models loaded successfully!")
    return small_model, medium_model, tokenizer


def calculate_log_probability(model: GPT2, tokenizer: GPT2Tokenizer, text: str, device: str) -> tuple[float, float, int]:
    """Calculate log probability of text under a model"""
    # Tokenize
    inputs = torch.tensor([tokenizer.encode(text)]).to(device)
    
    if len(inputs[0]) <= 1:
        return float("-inf"), float("-inf"), 0
    
    with torch.no_grad():
        # Pass targets to get logits for all positions
        logits, _ = model(inputs, targets=inputs)
        
        # Shift for next token prediction
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = inputs[..., 1:].contiguous()
        
        # Calculate log probabilities
        log_probs = F.log_softmax(shift_logits, dim=-1)
        
        # Get log probabilities for actual tokens
        token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)
        
        # Total and per-token log probability
        total_log_prob = token_log_probs.sum().item()
        per_token_log_prob = token_log_probs.mean().item()
        token_count = len(shift_labels[0])
    
    return total_log_prob, per_token_log_prob, token_count


def run_meta_generation_experiment(prompt: str = "The future of artificial intelligence is", 
                                 num_generations: int = 100,
                                 max_length: int = 30,
                                 temperature: float = 1.0) -> List[GenerationResult]:
    """Run the simplified meta-generation experiment"""
    
    # Load models
    small_model, medium_model, tokenizer = load_models()
    device = next(small_model.parameters()).device
    
    print(f"Generating {num_generations} outputs from small model...")
    print(f"Prompt: '{prompt}'")
    print(f"Temperature: {temperature}, Max length: {max_length}")
    print("-" * 60)
    
    results = []
    
    for i in range(num_generations):
        if (i + 1) % 10 == 0:
            print(f"Progress: {i + 1}/{num_generations}")
        
        # Generate text using small model
        # Ensure model is on correct device
        device = next(small_model.parameters()).device
        input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)
        
        small_model.eval()
        with torch.no_grad():
            for _ in range(max_length):
                logits, _ = small_model(input_ids)
                next_token_logits = logits[0, -1, :]
                next_token = temperature_sample(next_token_logits, temperature)
                input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
        
        generated_text = tokenizer.decode(input_ids[0].tolist())
        
        # Remove prompt from generated text
        generated_only = generated_text[len(prompt):].strip()
        
        if not generated_only:
            continue
        
        # Calculate probabilities under both models
        full_text = prompt + " " + generated_only
        
        # Small model evaluation
        small_total_log_prob, small_per_token_log_prob, token_count = calculate_log_probability(
            small_model, tokenizer, full_text, device
        )
        
        # Medium model evaluation
        medium_total_log_prob, medium_per_token_log_prob, _ = calculate_log_probability(
            medium_model, tokenizer, full_text, device
        )
        
        result = GenerationResult(
            prompt=prompt,
            generated_text=generated_only,
            small_log_prob=small_total_log_prob,
            medium_log_prob=medium_total_log_prob,
            small_per_token_log_prob=small_per_token_log_prob,
            medium_per_token_log_prob=medium_per_token_log_prob,
            token_count=token_count
        )
        
        results.append(result)
    
    print(f"\nGenerated {len(results)} valid outputs")
    return results


def analyze_and_sort_results(results: List[GenerationResult]) -> None:
    """Analyze and sort results by different criteria"""
    
    print("\n" + "="*60)
    print("ANALYSIS: Sorting by Higher Log Probability (Both Models)")
    print("="*60)
    
    # Sort by outputs that have higher log probability according to both models
    # We'll use the sum of both log probabilities as the sorting criterion
    sorted_by_both_total = sorted(results, 
                                key=lambda r: r.small_log_prob + r.medium_log_prob, 
                                reverse=True)
    
    print("\nTop 10 outputs with highest combined total log probability:")
    for i, result in enumerate(sorted_by_both_total[:10]):
        print(f"\n{i+1}. Text: '{result.generated_text[:80]}...'")
        print(f"   Small total log prob: {result.small_log_prob:.3f}")
        print(f"   Medium total log prob: {result.medium_log_prob:.3f}")
        print(f"   Combined: {result.small_log_prob + result.medium_log_prob:.3f}")
    
    print("\n" + "="*60)
    print("ANALYSIS: Sorting by Higher Per-Token Log Probability (Both Models)")
    print("="*60)
    
    # Sort by outputs that have higher per-token log probability according to both models
    sorted_by_both_per_token = sorted(results, 
                                    key=lambda r: r.small_per_token_log_prob + r.medium_per_token_log_prob, 
                                    reverse=True)
    
    print("\nTop 10 outputs with highest combined per-token log probability:")
    for i, result in enumerate(sorted_by_both_per_token[:10]):
        print(f"\n{i+1}. Text: '{result.generated_text[:80]}...'")
        print(f"   Small per-token log prob: {result.small_per_token_log_prob:.3f}")
        print(f"   Medium per-token log prob: {result.medium_per_token_log_prob:.3f}")
        print(f"   Combined: {result.small_per_token_log_prob + result.medium_per_token_log_prob:.3f}")
    
    # Additional analysis
    print("\n" + "="*60)
    print("ADDITIONAL ANALYSIS")
    print("="*60)
    
    # Compare model preferences
    small_better_count = sum(1 for r in results if r.small_per_token_log_prob > r.medium_per_token_log_prob)
    medium_better_count = len(results) - small_better_count
    
    print(f"\nModel preference comparison (per-token log probability):")
    print(f"Small model assigns higher probability: {small_better_count}/{len(results)} ({small_better_count/len(results)*100:.1f}%)")
    print(f"Medium model assigns higher probability: {medium_better_count}/{len(results)} ({medium_better_count/len(results)*100:.1f}%)")
    
    # Show examples where models disagree most
    disagreement_scores = [(abs(r.small_per_token_log_prob - r.medium_per_token_log_prob), r) 
                          for r in results]
    disagreement_scores.sort(reverse=True)
    
    print(f"\nTop 3 examples where models disagree most:")
    for i, (disagreement, result) in enumerate(disagreement_scores[:3]):
        preferred_model = "Small" if result.small_per_token_log_prob > result.medium_per_token_log_prob else "Medium"
        print(f"\n{i+1}. Text: '{result.generated_text[:60]}...'")
        print(f"   Small: {result.small_per_token_log_prob:.3f}, Medium: {result.medium_per_token_log_prob:.3f}")
        print(f"   Disagreement: {disagreement:.3f} (prefers: {preferred_model})")


def main():
    """Main function to run the simplified meta-generation experiment"""
    print("SIMPLIFIED META-GENERATION EXPERIMENT")
    print("="*60)
    
    # Run experiment
    results = run_meta_generation_experiment(
        prompt="The future of artificial intelligence is",
        num_generations=100,
        max_length=30,
        temperature=1.0
    )
    
    # Analyze and sort results
    analyze_and_sort_results(results)
    
    print("\n" + "="*60)
    print("EXPERIMENT COMPLETE!")
    print("="*60)


if __name__ == "__main__":
    main()

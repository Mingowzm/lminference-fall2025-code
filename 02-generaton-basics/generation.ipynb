{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4202c321",
   "metadata": {},
   "source": [
    "# Temperature Sampling with GPT-2\n",
    "\n",
    "By Graham Neubig for [11-664/763 Inference Algorithms for Language Modeling](https://phontron.com/class/lminference-fall2025/)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neubig/lminference-fall2025-code/02-generation-basics/2025-09-02-sampling-methods/generation.ipynb)\n",
    "\n",
    "This notebook explores temperature sampling, the most fundamental technique for controlling randomness in text generation. Temperature sampling modifies the probability distribution over next tokens by scaling the logits before applying softmax.\n",
    "\n",
    "**Temperature Values and Their Effects:**\n",
    "- **T = 0.0**: Greedy sampling (deterministic, always picks most likely token)\n",
    "- **T = 0.5**: Conservative sampling (focused, coherent, less creative)\n",
    "- **T = 1.0**: Standard sampling (balanced creativity and coherence)\n",
    "- **T = 1.5**: Creative sampling (diverse, more surprising outputs)\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "Temperature sampling scales logits by 1/T before applying softmax: `P(token) = softmax(logits / T)`\n",
    "- Lower T makes the distribution more peaked (focused on likely tokens)\n",
    "- Higher T makes the distribution more uniform (considers more options)\n",
    "- T = 0 reduces to greedy decoding (argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from litellm import completion  # type: ignore\n",
    "except ImportError:\n",
    "    completion = None\n",
    "\n",
    "from nanogpt import GPT2, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8ec5d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 1. Text Generation with Temperature Sampling\n",
    "\n",
    "The core implementation involves two key functions:\n",
    "1. `temperature_sample()`: Applies temperature scaling to logits and samples a token\n",
    "2. `generate_with_temperature()`: Generates a complete text sequence using temperature sampling\n",
    "\n",
    "Temperature scaling works by dividing logits by the temperature value before applying softmax.\n",
    "This changes the \"sharpness\" of the probability distribution without changing the relative ordering of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8789db",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def temperature_sample(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample from logits using temperature scaling.\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model outputs [vocab_size]\n",
    "        temperature: Sampling temperature (0.0 = greedy, higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        Sampled token index\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "        # Greedy sampling: always pick the most likely token\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Scale logits by temperature and sample\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "\n",
    "def generate_with_temperature(\n",
    "    model: GPT2,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    prompt: str,\n",
    "    temperature: float,\n",
    "    max_length: int = 30,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with specified temperature.\n",
    "\n",
    "    Args:\n",
    "        model: GPT-2 model\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        prompt: Input text to continue\n",
    "        temperature: Sampling temperature\n",
    "        max_length: Maximum number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Generated text (including original prompt)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits, _ = model(input_ids)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token = temperature_sample(next_token_logits, temperature)\n",
    "            input_ids = torch.cat(\n",
    "                [input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1\n",
    "            )\n",
    "\n",
    "    return tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Run the temperature demonstration and evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4594f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Demonstrate temperature effects with real GPT-2 model\n",
    "    print(\"Loading GPT-2 model...\")\n",
    "    model = GPT2.from_pretrained(\"gpt2\")\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    tokenizer = GPT2Tokenizer()\n",
    "\n",
    "    prompt = \"The future of artificial intelligence is\"\n",
    "    temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "    for temp in temperatures:\n",
    "        generated = generate_with_temperature(\n",
    "            model, tokenizer, prompt, temp, max_length=25\n",
    "        )\n",
    "        generated_part = generated[len(prompt) :].strip()\n",
    "        temp_label = \"Greedy\" if temp == 0.0 else f\"T={temp}\"\n",
    "        print(f\"{temp_label:8}: {generated_part}\")\n",
    "\n",
    "    # Run evaluation and show results\n",
    "    try:\n",
    "        results = run_evaluation()\n",
    "\n",
    "        # Group by temperature and calculate averages\n",
    "        temp_groups = {}\n",
    "        for result in results:\n",
    "            if result.temperature not in temp_groups:\n",
    "                temp_groups[result.temperature] = []\n",
    "            temp_groups[result.temperature].append(result)\n",
    "\n",
    "        print(\"\\nAverage metrics by temperature:\")\n",
    "        for temp in sorted(temp_groups.keys()):\n",
    "            group = temp_groups[temp]\n",
    "            avg_diversity = sum(r.diversity for r in group) / len(group)\n",
    "            avg_time = sum(r.generation_time for r in group) / len(group)\n",
    "            avg_fluency = sum(r.fluency_score for r in group) / len(group)\n",
    "\n",
    "            temp_label = \"Greedy\" if temp == 0.0 else f\"T={temp}\"\n",
    "            print(\n",
    "                f\"{temp_label:8}: Diversity={avg_diversity:.3f}, Time={avg_time:.3f}s, Fluency={avg_fluency:.1f}/10\"\n",
    "            )\n",
    "\n",
    "        print(\"\\nSample outputs:\")\n",
    "        for temp in sorted(temp_groups.keys()):\n",
    "            example = temp_groups[temp][0]\n",
    "            temp_label = \"Greedy\" if temp == 0.0 else f\"T={temp}\"\n",
    "            print(\n",
    "                f\"{temp_label}: '{example.generated_text[:50]}...' (Fluency: {example.fluency_score:.1f})\"\n",
    "            )\n",
    "\n",
    "        print(\"\\nKey Findings:\")\n",
    "        print(\"- Diversity generally increases with temperature\")\n",
    "        print(\"- Generation speed is relatively consistent across temperatures\")\n",
    "        print(\"- Fluency scores help identify the optimal temperature range\")\n",
    "\n",
    "        # Find optimal temperature based on fluency\n",
    "        best_temp = max(\n",
    "            temp_groups.keys(),\n",
    "            key=lambda t: sum(r.fluency_score for r in temp_groups[t])\n",
    "            / len(temp_groups[t]),\n",
    "        )\n",
    "        best_fluency = sum(r.fluency_score for r in temp_groups[best_temp]) / len(\n",
    "            temp_groups[best_temp]\n",
    "        )\n",
    "        print(f\"- Highest average fluency: T={best_temp} ({best_fluency:.1f}/10)\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Configuration Error: {e}\")\n",
    "        print(\"\\nTo run fluency evaluation, set these environment variables:\")\n",
    "        print(\"- LLM_API_KEY: Your API key\")\n",
    "        print(\"- LLM_MODEL: Model name (required)\")\n",
    "        print(\"- LLM_BASE_URL: Base URL (optional, for custom endpoints)\")\n",
    "        print(\"\\nExample:\")\n",
    "        print(\"export LLM_API_KEY='your-api-key-here'\")\n",
    "        print(\"export LLM_MODEL='gpt-3.5-turbo'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation Error: {e}\")\n",
    "        print(\"Check your API configuration and try again.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc385235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Evaluation and Analysis\n",
    "\n",
    "To systematically compare temperature settings, we need quantitative metrics:\n",
    "1. **Diversity**: Ratio of unique words to total words (higher = more diverse)\n",
    "2. **Generation Speed**: Time taken to generate text\n",
    "3. **Fluency**: Quality assessment using an external API (optional)\n",
    "\n",
    "We'll run experiments across multiple prompts and temperature values to understand the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb354ac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationResult:\n",
    "    \"\"\"Results from a text generation experiment.\"\"\"\n",
    "\n",
    "    temperature: float\n",
    "    prompt: str\n",
    "    generated_text: str\n",
    "    diversity: float\n",
    "    generation_time: float\n",
    "    fluency_score: float\n",
    "\n",
    "\n",
    "def calculate_diversity(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate text diversity as unique word ratio.\n",
    "\n",
    "    Args:\n",
    "        text: Generated text to analyze\n",
    "\n",
    "    Returns:\n",
    "        Ratio of unique words to total words (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "def evaluate_fluency(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate text fluency using an external API.\n",
    "\n",
    "    Args:\n",
    "        text: Text to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Fluency score (0-10)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required environment variables are not set\n",
    "        Exception: If API call fails\n",
    "    \"\"\"\n",
    "    # Get configuration from environment variables\n",
    "    api_key = os.getenv(\"LLM_API_KEY\")\n",
    "    model = os.getenv(\"LLM_MODEL\")\n",
    "    base_url = os.getenv(\"LLM_BASE_URL\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\n",
    "            \"LLM_API_KEY environment variable is required for fluency evaluation\"\n",
    "        )\n",
    "\n",
    "    if not model:\n",
    "        raise ValueError(\n",
    "            \"LLM_MODEL environment variable is required for fluency evaluation\"\n",
    "        )\n",
    "\n",
    "    if completion is None:\n",
    "        raise ValueError(\"litellm package is required for fluency evaluation\")\n",
    "\n",
    "    try:\n",
    "        # Prepare completion arguments\n",
    "        completion_args = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Rate the fluency and coherence of this text on a scale of 0-10 (10 = perfect). Only respond with a number: '{text}'\",\n",
    "                }\n",
    "            ],\n",
    "            \"api_key\": api_key,\n",
    "            \"max_tokens\": 5,\n",
    "        }\n",
    "\n",
    "        # Add base URL if provided\n",
    "        if base_url:\n",
    "            completion_args[\"base_url\"] = base_url\n",
    "\n",
    "        response = completion(**completion_args)\n",
    "        score_text = response.choices[0].message.content.strip()  # type: ignore\n",
    "\n",
    "        # Extract numeric score\n",
    "        try:\n",
    "            score = float(score_text)\n",
    "            # Clamp score to valid range\n",
    "            return max(0.0, min(10.0, score))\n",
    "        except ValueError:\n",
    "            # If response isn't a number, try to extract it\n",
    "            import re\n",
    "\n",
    "            numbers = re.findall(r\"\\d+\\.?\\d*\", score_text)\n",
    "            if numbers:\n",
    "                score = float(numbers[0])\n",
    "                return max(0.0, min(10.0, score))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Could not parse fluency score from response: {score_text}\"\n",
    "                ) from None\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Fluency evaluation failed: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def run_evaluation() -> list[GenerationResult]:\n",
    "    \"\"\"\n",
    "    Run systematic evaluation across temperatures and prompts.\n",
    "\n",
    "    Returns:\n",
    "        List of GenerationResult objects\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If LLM configuration is missing\n",
    "    \"\"\"\n",
    "    # Check for required LLM configuration\n",
    "    api_key = os.getenv(\"LLM_API_KEY\")\n",
    "    model_name = os.getenv(\"LLM_MODEL\")\n",
    "    base_url = os.getenv(\"LLM_BASE_URL\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"LLM_API_KEY environment variable is required for evaluation\")\n",
    "\n",
    "    if not model_name:\n",
    "        raise ValueError(\"LLM_MODEL environment variable is required for evaluation\")\n",
    "\n",
    "    print(f\"Using LLM: {model_name}\")\n",
    "    if base_url:\n",
    "        print(f\"Base URL: {base_url}\")\n",
    "\n",
    "    # Load GPT-2 model and tokenizer\n",
    "    model = GPT2.from_pretrained(\"gpt2\")\n",
    "    tokenizer = GPT2Tokenizer()\n",
    "\n",
    "    # Test prompts representing different use cases\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"Once upon a time in a magical forest\",\n",
    "        \"Machine learning algorithms work by\",\n",
    "        \"The most important factor in climate change is\",\n",
    "    ]\n",
    "\n",
    "    temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "    results = []\n",
    "\n",
    "    print(\"Running evaluation across prompts and temperatures...\")\n",
    "\n",
    "    for prompt in prompts:\n",
    "        for temp in temperatures:\n",
    "            print(f\"  Evaluating T={temp} with prompt: '{prompt[:30]}...'\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            generated = generate_with_temperature(\n",
    "                model, tokenizer, prompt, temp, max_length=20\n",
    "            )\n",
    "            generation_time = time.time() - start_time\n",
    "\n",
    "            generated_part = generated[len(prompt) :].strip()\n",
    "            diversity = calculate_diversity(generated_part)\n",
    "            fluency_score = evaluate_fluency(generated_part)\n",
    "\n",
    "            results.append(\n",
    "                GenerationResult(\n",
    "                    temperature=temp,\n",
    "                    prompt=prompt,\n",
    "                    generated_text=generated_part,\n",
    "                    diversity=diversity,\n",
    "                    generation_time=generation_time,\n",
    "                    fluency_score=fluency_score,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825907d",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "**Temperature Selection Guide:**\n",
    "- **T = 0.0 (Greedy)**: Deterministic, focused, potentially repetitive\n",
    "- **T = 0.5**: Conservative, coherent, good for factual content\n",
    "- **T = 1.0**: Balanced creativity and coherence, good default\n",
    "- **T = 1.5**: Creative, diverse, good for creative writing\n",
    "\n",
    "**Key Trade-offs:**\n",
    "- Lower temperature → Higher coherence, lower diversity\n",
    "- Higher temperature → Lower coherence, higher diversity\n",
    "- The optimal temperature depends on your specific application\n",
    "\n",
    "**Practical Recommendations:**\n",
    "1. Start with T = 1.0 as a baseline for most applications\n",
    "2. Use T = 0.0 (greedy) when you need deterministic, reproducible outputs\n",
    "3. Lower temperature (0.3-0.7) for factual content, documentation, or technical writing\n",
    "4. Higher temperature (1.0-1.5) for creative writing, brainstorming, or content generation\n",
    "5. Very high temperatures (>2.0) often produce incoherent text and should be used sparingly\n",
    "6. Always evaluate on your specific use case - optimal temperature varies by domain and application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

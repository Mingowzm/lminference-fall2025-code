{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install 'litellm[proxy]'"
      ],
      "metadata": {
        "id": "Plpxr_zipejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "WSxyieLGiGfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup vllm serve Qwen/Qwen3-1.7B --port 4000 --gpu-memory-utilization 0.8 > vllm.log 2>&1 &"
      ],
      "metadata": {
        "id": "3TJ87Uclitm4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4000/v1/models"
      ],
      "metadata": {
        "id": "rMX1bVz7pz0d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://127.0.0.1:4000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"model\":\"Qwen/Qwen3-1.7B\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in five words.\"}], \"max_tokens\": 64}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sGttdNCp5i3",
        "outputId": "3d660585-0dcb-4671-ef61-634d82a64c0a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\":\"chatcmpl-23ca0e0b708a4df7b5ca7bb258a49450\",\"object\":\"chat.completion\",\"created\":1758599344,\"model\":\"Qwen/Qwen3-1.7B\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"<think>\\nOkay, the user wants me to say hello in five words. Let me think about how to approach this. First, I need to make sure I understand the request correctly. They want a greeting that's concise, exactly five words. \\n\\nHmm, the most common way to say hello is \\\"Hello!\\\" but\",\"refusal\":null,\"annotations\":null,\"audio\":null,\"function_call\":null,\"tool_calls\":[],\"reasoning_content\":null},\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"token_ids\":null}],\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":14,\"total_tokens\":78,\"completion_tokens\":64,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"prompt_token_ids\":null,\"kv_transfer_params\":null}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at log file"
      ],
      "metadata": {
        "id": "TwlpxXheB_In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -200 vllm.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWZGIc6MjvJ3",
        "outputId": "1ea61bf9-1325-4f87-bfba-cd3e8d6d6ba1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-23 04:10:35.942818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758600635.973685    2025 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758600635.982735    2025 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758600636.003064    2025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758600636.003095    2025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758600636.003099    2025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758600636.003102    2025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-23 04:10:36.009595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 09-23 04:10:46 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m INFO 09-23 04:10:51 [api_server.py:1896] vLLM API server version 0.10.2\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m INFO 09-23 04:10:51 [utils.py:328] non-default args: {'model_tag': 'Qwen/Qwen3-1.7B', 'port': 4000, 'model': 'Qwen/Qwen3-1.7B', 'gpu_memory_utilization': 0.8}\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m INFO 09-23 04:11:08 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m WARNING 09-23 04:11:08 [__init__.py:2716] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m WARNING 09-23 04:11:08 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m INFO 09-23 04:11:08 [__init__.py:1815] Using max model len 40960\n",
            "\u001b[1;36m(APIServer pid=2025)\u001b[0;0m INFO 09-23 04:11:11 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "2025-09-23 04:11:20.640036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758600680.673503    2300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758600680.683220    2300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758600680.706234    2300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758600680.706264    2300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758600680.706269    2300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758600680.706272    2300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO 09-23 04:11:25 [__init__.py:216] Automatically detected platform cuda.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:28 [core.py:654] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:28 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen3-1.7B', speculative_config=None, tokenizer='Qwen/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-1.7B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m ERROR 09-23 04:11:28 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "[W923 04:11:29.292496289 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:29 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m WARNING 09-23 04:11:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:29 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen3-1.7B...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:30 [gpu_model_runner.py:2370] Loading model from scratch...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:30 [cuda.py:368] Using FlexAttention backend on V1 engine.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=2300)\u001b[0;0m INFO 09-23 04:11:31 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"hosted_vllm/Qwen/Qwen3-1.7B\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"hello\"}],\n",
        "    api_base=\"http://127.0.0.1:4000/v1\",\n",
        "    api_key=\"sk-local\",  # this will work for local\n",
        "    temperature=0.2,\n",
        "    max_tokens=10,\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGQG01cmj063",
        "outputId": "9b1009f2-7d4f-47b8-8cc2-684a99c07fb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-8de5cb53939b455696880c88e714d9ac', created=1758601047, model='hosted_vllm/Qwen/Qwen3-1.7B', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='length', index=0, message=Message(content='<think>\\nOkay, the user said \"hello\".', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}), provider_specific_fields={'stop_reason': None, 'token_ids': None})], usage=Usage(completion_tokens=10, prompt_tokens=9, total_tokens=19, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None, prompt_logprobs=None, prompt_token_ids=None, kv_transfer_params=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import litellm\n",
        "\n",
        "api_base = \"http://127.0.0.1:4000/v1\"\n",
        "api_key = \"sk-local\"\n",
        "model = \"hosted_vllm/Qwen/Qwen3-1.7B\"\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"submit_paths\",\n",
        "            \"description\": \"Submit the top P shortest paths found\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}}},\n",
        "                    \"weights\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}},\n",
        "                },\n",
        "                \"required\": [\"paths\", \"weights\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "resp = litellm.completion(\n",
        "    model=model,\n",
        "    api_base=api_base,\n",
        "    api_key=api_key,\n",
        "    messages=[{\"role\":\"user\",\"content\":\"Return two example paths using submit_paths.\"}],\n",
        "    tools=tools,\n",
        "    tool_choice={\"type\":\"function\",\"function\":{\"name\":\"submit_paths\"}},\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "msg = resp.choices[0].message\n",
        "tool_calls = getattr(msg, \"tool_calls\", None) or msg.get(\"tool_calls\", [])\n",
        "if tool_calls:\n",
        "    args = json.loads(tool_calls[0][\"function\"][\"arguments\"])\n",
        "    print(\"Tool args:\", args)\n",
        "else:\n",
        "    print(\"No tool call returned:\", msg)"
      ],
      "metadata": {
        "id": "Yas2qtzuymLU",
        "outputId": "c3dc1a48-83a3-453e-fa49-2751b9929feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool args: {'paths': [[1, 2, 3], [1, 4, 3]], 'weights': [1, 2]}\n"
          ]
        }
      ]
    }
  ]
}
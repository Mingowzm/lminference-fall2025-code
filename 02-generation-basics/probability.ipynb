{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ade253f",
   "metadata": {},
   "source": [
    "# Probability Fundamentals for Language Models\n",
    "\n",
    "By Graham Neubig for [11-664/763 Inference Algorithms for Language Modeling](https://phontron.com/class/lminference-fall2025/)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neubig/lminference-fall2025-code/blob/main/02-generation-basics/probability.ipynb)\n",
    "\n",
    "This notebook explores the fundamental probability concepts that underlie language models and text generation. We'll cover key mathematical concepts and their PyTorch implementations, building intuition for how sampling strategies work in practice.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand joint and conditional probabilities and their role in language modeling\n",
    "- Work with latent variables and marginalization\n",
    "- Implement sampling strategies including temperature sampling\n",
    "- Use sampling-based approximation methods\n",
    "- Apply maximum likelihood estimation from samples\n",
    "- Visualize probability concepts with interactive examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bb9ce",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "First, let's install and import the necessary libraries. We'll use PyTorch for tensor operations and probability distributions, matplotlib for visualization, and numpy for numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb304e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install torch>=2.5.0 numpy>=2.0.0 matplotlib>=3.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e127c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from plotting_utils import (\n",
    "    create_subplot_grid,\n",
    "    plot_bar_chart,\n",
    "    plot_conditional_probability_heatmap,\n",
    "    plot_heatmap,\n",
    "    setup_axis_with_rotation,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77873437",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 1. Conditional Probability: The Foundation of Language Models\n",
    "\n",
    "Language models are fundamentally based on conditional probability. Given a sequence of words, we want to predict the next word. Mathematically, this is:\n",
    "\n",
    "$$P(w_{t+1} | w_1, w_2, \\ldots, w_t)$$\n",
    "\n",
    "The key relationship is: $$P(y|x) = \\frac{P(x,y)}{P(x)}$$\n",
    "\n",
    "Let's set up a bigram language model and visualize its conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5c76a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def setup_bigram_model() -> tuple[list[str], int, torch.Tensor, dict[str, int]]:\n",
    "    \"\"\"Set up a bigram language model with conditional probabilities.\"\"\"\n",
    "    vocab = [\n",
    "        \"<s>\",\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"cat\",\n",
    "        \"dog\",\n",
    "        \"park\",\n",
    "        \"mat\",\n",
    "        \"sat\",\n",
    "        \"ran\",\n",
    "        \"walked\",\n",
    "        \"played\",\n",
    "        \"on\",\n",
    "        \"in\",\n",
    "        \"to\",\n",
    "        \"quickly\",\n",
    "        \"slowly\",\n",
    "        \"</s>\",\n",
    "    ]\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create word-to-index mapping for clarity\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    # Define conditional probabilities P(next_word | current_word)\n",
    "    # These represent realistic language model transitions\n",
    "    conditional_probs = torch.zeros(vocab_size, vocab_size)\n",
    "\n",
    "    transitions = {\n",
    "        # Beginning of sentence typically followed by articles\n",
    "        \"<s>\": {\"the\": 0.4, \"a\": 0.3, \"cat\": 0.1, \"dog\": 0.1, \"park\": 0.1},\n",
    "        # Articles typically followed by nouns\n",
    "        \"the\": {\"cat\": 0.25, \"dog\": 0.25, \"park\": 0.25, \"mat\": 0.25},\n",
    "        \"a\": {\"cat\": 0.3, \"dog\": 0.3, \"park\": 0.2, \"mat\": 0.2},\n",
    "        # Nouns typically followed by verbs\n",
    "        \"cat\": {\"sat\": 0.3, \"ran\": 0.2, \"walked\": 0.2, \"played\": 0.3},\n",
    "        \"dog\": {\"ran\": 0.4, \"walked\": 0.3, \"played\": 0.2, \"sat\": 0.1},\n",
    "        \"park\": {\"walked\": 0.4, \"played\": 0.3, \"ran\": 0.2, \"sat\": 0.1},\n",
    "        \"mat\": {\"sat\": 0.6, \"ran\": 0.2, \"walked\": 0.1, \"played\": 0.1},\n",
    "        # Verbs can be followed by prepositions or adverbs\n",
    "        \"sat\": {\"on\": 0.4, \"in\": 0.2, \"quickly\": 0.15, \"slowly\": 0.15, \"</s>\": 0.1},\n",
    "        \"ran\": {\"to\": 0.3, \"in\": 0.2, \"quickly\": 0.2, \"slowly\": 0.1, \"</s>\": 0.2},\n",
    "        \"walked\": {\"to\": 0.3, \"in\": 0.2, \"quickly\": 0.15, \"slowly\": 0.15, \"</s>\": 0.2},\n",
    "        \"played\": {\"in\": 0.3, \"on\": 0.2, \"quickly\": 0.2, \"slowly\": 0.1, \"</s>\": 0.2},\n",
    "        # Prepositions typically followed by articles or nouns\n",
    "        \"on\": {\"the\": 0.5, \"a\": 0.2, \"mat\": 0.3},\n",
    "        \"in\": {\"the\": 0.6, \"a\": 0.4},\n",
    "        \"to\": {\"the\": 0.7, \"a\": 0.3},\n",
    "        # Adverbs can end sentences or be followed by prepositions\n",
    "        \"quickly\": {\"on\": 0.2, \"in\": 0.2, \"to\": 0.2, \"</s>\": 0.4},\n",
    "        \"slowly\": {\"on\": 0.2, \"in\": 0.2, \"to\": 0.2, \"</s>\": 0.4},\n",
    "        # End of sentence (for completeness, though not used in generation)\n",
    "        \"</s>\": {\"<s>\": 1.0},\n",
    "    }\n",
    "\n",
    "    # Fill in the conditional probability matrix\n",
    "    for current_word, next_words in transitions.items():\n",
    "        current_idx = word_to_idx[current_word]\n",
    "        for next_word, prob in next_words.items():\n",
    "            next_idx = word_to_idx[next_word]\n",
    "            conditional_probs[current_idx, next_idx] = prob\n",
    "\n",
    "    return vocab, vocab_size, conditional_probs, word_to_idx\n",
    "\n",
    "\n",
    "# Set up our bigram language model\n",
    "vocab, vocab_size, conditional_probs, word_to_idx = setup_bigram_model()\n",
    "\n",
    "# Visualize the conditional probabilities with a heatmap\n",
    "plot_conditional_probability_heatmap(\n",
    "    conditional_probs.numpy(),\n",
    "    vocab,\n",
    "    title=\"Conditional P(next_word | current_word)\",\n",
    "    xlabel=\"Next Word\",\n",
    "    ylabel=\"Current Word\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ed9ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Sampling from Language Models\n",
    "\n",
    "Now that we have our conditional probabilities, let's see how we can use them to generate text sequences. This demonstrates the core sampling process used in language model text generation.\n",
    "\n",
    "The sampling process follows this pattern:\n",
    "1. Start with a beginning token\n",
    "2. Use conditional probabilities to sample the next word\n",
    "3. Update the current context and repeat\n",
    "4. Continue until we reach an end token or maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0885df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_bigram_model(\n",
    "    conditional_probs: torch.Tensor,\n",
    "    word_to_idx: dict[str, int],\n",
    "    num_sequences: int = 1000,\n",
    "    max_length: int = 10,\n",
    "    temperature: float = 1.0,\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Sample sequences from the bigram model with optional temperature control.\n",
    "    \"\"\"\n",
    "    vocab = list(word_to_idx.keys())\n",
    "    sequences = []\n",
    "\n",
    "    start_idx = word_to_idx[\"<s>\"]\n",
    "    end_idx = word_to_idx[\"</s>\"]\n",
    "\n",
    "    for _ in range(num_sequences):\n",
    "        sequence = [\"<s>\"]\n",
    "        current_idx = start_idx\n",
    "\n",
    "        for _ in range(max_length - 1):\n",
    "            # Get next word probabilities and apply temperature\n",
    "            next_probs = conditional_probs[current_idx]\n",
    "\n",
    "            if temperature != 1.0:\n",
    "                # Apply temperature: P_T(x) = softmax(log P(x) / T)\n",
    "                logits = torch.log(next_probs + 1e-8)  # Add small epsilon to avoid log(0)\n",
    "                next_probs = F.softmax(logits / temperature, dim=0)\n",
    "\n",
    "            # Sample next word\n",
    "            next_idx = int(torch.multinomial(next_probs, 1).item())\n",
    "            next_word = vocab[next_idx]\n",
    "            sequence.append(next_word)\n",
    "\n",
    "            if next_idx == end_idx:\n",
    "                break\n",
    "\n",
    "            current_idx = next_idx\n",
    "\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def demonstrate_sampling() -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Demonstrate sampling from the bigram model and show example sentences.\n",
    "    \"\"\"\n",
    "    # Generate a small number of sequences for demonstration\n",
    "    sequences = sample_from_bigram_model(conditional_probs, word_to_idx, num_sequences=20, max_length=8)\n",
    "\n",
    "    # Show a few example sentences\n",
    "    for i in range(3):\n",
    "        if i < len(sequences):\n",
    "            clean_seq = [w for w in sequences[i] if w not in [\"<s>\", \"</s>\"]]\n",
    "            print(f\"{' '.join(clean_seq)}\")\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# Run the sampling demonstration\n",
    "sample_sequences = demonstrate_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f5f93",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3. Sampling-Based Estimation: Using Sampling to Estimate Unknown Quantities\n",
    "\n",
    "**Key Principle**: Sampling is a powerful method for estimating unknown quantities when we have access to related known distributions.\n",
    "\n",
    "In our case:\n",
    "- **Known**: True conditional probabilities P(next_word | current_word) from our bigram model\n",
    "- **Unknown**: What these conditional probabilities would be if estimated from limited data\n",
    "- **Method**: Generate samples from the known conditional distribution and use them to estimate the conditional probabilities\n",
    "\n",
    "This demonstrates the fundamental sampling principle:\n",
    "$$P(y|x) = \\lim_{N \\to \\infty} \\frac{\\text{count}(x, y)}{\\text{count}(x)}$$\n",
    "\n",
    "**Why is this useful?** In real-world scenarios, we often need to estimate conditional probabilities from limited data. This shows how estimation accuracy improves with more samples and helps us understand the convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f353b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_joint_probabilities_from_sequences(\n",
    "    sequences: list[list[int]], vocab_size: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Estimate joint probabilities from sampled sequences.\n",
    "    \"\"\"\n",
    "    bigram_counts = torch.zeros((vocab_size, vocab_size))\n",
    "    total_bigrams = 0\n",
    "\n",
    "    # Count bigrams from sequences\n",
    "    for sequence in sequences:\n",
    "        for i in range(len(sequence) - 1):\n",
    "            current_idx = sequence[i]\n",
    "            next_idx = sequence[i + 1]\n",
    "            bigram_counts[current_idx, next_idx] += 1\n",
    "            total_bigrams += 1\n",
    "\n",
    "    # Estimate joint probabilities from counts\n",
    "    joint_prob_estimated = bigram_counts / total_bigrams if total_bigrams > 0 else bigram_counts\n",
    "\n",
    "    return bigram_counts, joint_prob_estimated\n",
    "\n",
    "\n",
    "def estimate_conditional_probabilities_from_sequences(\n",
    "    sequences: list[list[int]], vocab_size: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Estimate conditional probabilities P(next_word | current_word) from sampled sequences.\n",
    "    \"\"\"\n",
    "    bigram_counts = torch.zeros((vocab_size, vocab_size))\n",
    "    current_word_counts = torch.zeros(vocab_size)\n",
    "\n",
    "    # Count bigrams and current words from sequences\n",
    "    for sequence in sequences:\n",
    "        for i in range(len(sequence) - 1):\n",
    "            current_idx = sequence[i]\n",
    "            next_idx = sequence[i + 1]\n",
    "            bigram_counts[current_idx, next_idx] += 1\n",
    "            current_word_counts[current_idx] += 1\n",
    "\n",
    "    # Estimate conditional probabilities P(next_word | current_word)\n",
    "    conditional_prob_estimated = torch.zeros((vocab_size, vocab_size))\n",
    "    for i in range(vocab_size):\n",
    "        if current_word_counts[i] > 0:\n",
    "            conditional_prob_estimated[i, :] = bigram_counts[i, :] / current_word_counts[i]\n",
    "\n",
    "    return bigram_counts, conditional_prob_estimated\n",
    "\n",
    "\n",
    "def demonstrate_sampling_based_estimation() -> (\n",
    "    tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        list[int],\n",
    "        list[dict[str, float]],\n",
    "        list[float],\n",
    "        dict[str, float],\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Demonstrate how to estimate conditional probabilities through sampling from known conditionals.\n",
    "    Shows convergence of specific bigram probability estimates and overall estimation error.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample with different numbers of sequences to show convergence\n",
    "    sample_sizes = [10, 50, 100, 500, 1000, 5000, 10000]\n",
    "\n",
    "    # Select a few interesting bigrams to track\n",
    "    target_bigrams = [(\"the\", \"cat\"), (\"cat\", \"sat\"), (\"on\", \"the\"), (\"sat\", \"on\")]\n",
    "\n",
    "    # Convert to indices and get true probabilities\n",
    "    target_indices = []\n",
    "    true_probabilities = {}\n",
    "    for w1, w2 in target_bigrams:\n",
    "        if w1 in word_to_idx and w2 in word_to_idx:\n",
    "            i1, i2 = word_to_idx[w1], word_to_idx[w2]\n",
    "            target_indices.append((i1, i2))\n",
    "            true_prob = conditional_probs[i1, i2].item()\n",
    "            true_probabilities[f\"{w1}_{w2}\"] = true_prob\n",
    "\n",
    "    # Track convergence of these bigrams and overall error\n",
    "    convergence_data = []\n",
    "    mean_errors = []\n",
    "\n",
    "    # Initialize variables to avoid unbound variable errors\n",
    "    conditional_prob_estimated = torch.zeros((vocab_size, vocab_size))\n",
    "    bigram_counts = torch.zeros((vocab_size, vocab_size))\n",
    "\n",
    "    for num_sequences in sample_sizes:\n",
    "        # Use the existing sampling function from section 2\n",
    "        sequences = sample_from_bigram_model(\n",
    "            conditional_probs,\n",
    "            word_to_idx,\n",
    "            num_sequences=num_sequences,\n",
    "            max_length=8,\n",
    "        )\n",
    "        # Convert string sequences to index sequences for estimation\n",
    "        sequences_idx = [[word_to_idx[word] for word in seq] for seq in sequences]\n",
    "        # Estimate conditional probabilities from the sequences\n",
    "        bigram_counts, conditional_prob_estimated = estimate_conditional_probabilities_from_sequences(\n",
    "            sequences_idx, vocab_size\n",
    "        )\n",
    "\n",
    "        # Track specific bigram probabilities\n",
    "        bigram_estimates = {}\n",
    "        for (w1, w2), (i1, i2) in zip(target_bigrams, target_indices):\n",
    "            prob = conditional_prob_estimated[i1, i2].item()\n",
    "            bigram_estimates[f\"{w1}_{w2}\"] = prob\n",
    "\n",
    "        convergence_data.append(bigram_estimates)\n",
    "\n",
    "        # Calculate mean error across all bigrams with non-zero true probability\n",
    "        errors = []\n",
    "        for i in range(vocab_size):\n",
    "            for j in range(vocab_size):\n",
    "                true_prob = conditional_probs[i, j].item()\n",
    "                if true_prob > 0:  # Only consider bigrams that actually exist\n",
    "                    estimated_prob = conditional_prob_estimated[i, j].item()\n",
    "                    error = abs(estimated_prob - true_prob)\n",
    "                    errors.append(error)\n",
    "\n",
    "        mean_error = sum(errors) / len(errors) if errors else 0.0\n",
    "        mean_errors.append(mean_error)\n",
    "\n",
    "    return (\n",
    "        conditional_prob_estimated,\n",
    "        bigram_counts,\n",
    "        sample_sizes,\n",
    "        convergence_data,\n",
    "        mean_errors,\n",
    "        true_probabilities,\n",
    "    )\n",
    "\n",
    "\n",
    "# Run the combined sampling-based estimation\n",
    "(\n",
    "    conditional_prob,\n",
    "    bigram_counts,\n",
    "    sample_sizes,\n",
    "    convergence_data,\n",
    "    mean_errors,\n",
    "    true_probabilities,\n",
    ") = demonstrate_sampling_based_estimation()\n",
    "\n",
    "# Create two-panel visualization: individual bigram convergence and overall error\n",
    "fig, axes = create_subplot_grid(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left panel: Individual bigram convergence\n",
    "bigram_names = list(convergence_data[0].keys())\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\"]\n",
    "\n",
    "for i, bigram_name in enumerate(bigram_names):\n",
    "    probabilities = [data[bigram_name] for data in convergence_data]\n",
    "    color = colors[i % len(colors)]\n",
    "\n",
    "    # Plot estimated probabilities\n",
    "    axes[0].plot(\n",
    "        sample_sizes,\n",
    "        probabilities,\n",
    "        \"o-\",\n",
    "        color=color,\n",
    "        linewidth=2,\n",
    "        markersize=6,\n",
    "        label=f\"Est P({bigram_name.replace('_', ' | ')})\",\n",
    "    )\n",
    "\n",
    "    # Plot true probability as horizontal line\n",
    "    true_prob = true_probabilities[bigram_name]\n",
    "    axes[0].axhline(\n",
    "        y=true_prob,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        linewidth=1,\n",
    "        label=f\"True P({bigram_name.replace('_', ' | ')}) = {true_prob:.3f}\",\n",
    "    )\n",
    "\n",
    "# Setup left panel with utility function\n",
    "setup_axis_with_rotation(\n",
    "    axes[0],\n",
    "    title=\"Individual Conditional Probability Convergence\",\n",
    "    xlabel=\"Number of Sequences (log scale)\",\n",
    "    ylabel=\"Estimated Conditional Probability\",\n",
    "    fontsize=12,\n",
    ")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale(\"log\")\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Right panel: Overall mean error convergence\n",
    "axes[1].plot(\n",
    "    sample_sizes,\n",
    "    mean_errors,\n",
    "    \"o-\",\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    label=\"Mean Absolute Error\",\n",
    ")\n",
    "\n",
    "# Setup right panel with utility function\n",
    "setup_axis_with_rotation(\n",
    "    axes[1],\n",
    "    title=\"Overall Estimation Error Convergence\",\n",
    "    xlabel=\"Number of Sequences (log scale)\",\n",
    "    ylabel=\"Mean Absolute Error\",\n",
    "    fontsize=12,\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the final estimated conditional probability heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "step = max(1, len(vocab) // 8)\n",
    "tick_positions = range(0, len(vocab), step)\n",
    "tick_labels = [vocab[i] for i in tick_positions]\n",
    "\n",
    "plot_heatmap(\n",
    "    conditional_prob.numpy(),\n",
    "    title=\"Final Estimated Conditional P(next_word | current_word)\",\n",
    "    cmap=\"Blues\",\n",
    "    xlabel=\"Next Word\",\n",
    "    ylabel=\"Current Word\",\n",
    "    xticks=tick_positions,\n",
    "    yticks=tick_positions,\n",
    "    xticklabels=tick_labels,\n",
    "    yticklabels=tick_labels,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68b521",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 4. Marginalization: Summing Over Variables\n",
    "\n",
    "Marginalization allows us to compute the probability of one variable by summing over all possible values of other variables. This is crucial for working with complex probability distributions.\n",
    "\n",
    "$$P(x) = \\sum_y P(x, y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the marginalization demonstration\n",
    "# For this section, we need joint probabilities, so let's compute them from our estimated conditional probabilities\n",
    "# We'll use the final conditional probabilities from section 3 and compute joint probabilities\n",
    "# by sampling to get the marginal distribution of current words\n",
    "\n",
    "# Sample a large number of sequences to get a good estimate of the marginal distribution\n",
    "large_sample_sequences = sample_from_bigram_model(conditional_probs, word_to_idx, num_sequences=10000, max_length=8)\n",
    "large_sample_sequences_idx = [[word_to_idx[word] for word in seq] for seq in large_sample_sequences]\n",
    "\n",
    "# Get both joint and conditional probabilities from the large sample\n",
    "_, joint_prob = estimate_joint_probabilities_from_sequences(large_sample_sequences_idx, vocab_size)\n",
    "\n",
    "marginal_x_computed = torch.sum(joint_prob, dim=1)\n",
    "marginal_y_computed = torch.sum(joint_prob, dim=0)\n",
    "\n",
    "# Visualize marginalization\n",
    "fig, axes = create_subplot_grid(1, 3, figsize=(15, 4))\n",
    "# Show only a subset of labels for readability\n",
    "step = max(1, len(vocab) // 8)\n",
    "tick_positions = range(0, len(vocab), step)\n",
    "tick_labels = [vocab[i] for i in tick_positions]\n",
    "\n",
    "plot_heatmap(\n",
    "    joint_prob.numpy(),\n",
    "    title=\"Joint P(current_word, next_word)\",\n",
    "    cmap=\"Blues\",\n",
    "    xlabel=\"Next Word\",\n",
    "    ylabel=\"Current Word\",\n",
    "    xticks=tick_positions,\n",
    "    yticks=tick_positions,\n",
    "    xticklabels=tick_labels,\n",
    "    yticklabels=tick_labels,\n",
    "    ax=axes[0],\n",
    ")\n",
    "\n",
    "plot_bar_chart(\n",
    "    marginal_x_computed.numpy(),\n",
    "    title=\"Marginal P(current_word)\",\n",
    "    xlabel=\"Word\",\n",
    "    xticks=tick_positions,\n",
    "    xticklabels=tick_labels,\n",
    "    ax=axes[1],\n",
    ")\n",
    "\n",
    "plot_bar_chart(\n",
    "    marginal_y_computed.numpy(),\n",
    "    title=\"Marginal P(next_word)\",\n",
    "    xlabel=\"Word\",\n",
    "    xticks=tick_positions,\n",
    "    xticklabels=tick_labels,\n",
    "    ax=axes[2],\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d54e54",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 5. Latent Variables: Hidden Structure in Models\n",
    "\n",
    "Latent variables are unobserved variables that help explain the relationship between observed variables.\n",
    "In this example, we'll model sequences where:\n",
    "- **x** = first word in the sequence (after \"&lt;s\")\n",
    "- **y** = last word in the sequence (before \"&lt;/s&gt;\")\n",
    "- **z** = everything else in between (the latent variable we marginalize over)\n",
    "\n",
    "The key insight: P(x, y) = Σ_z P(x, y, z) = Σ_z P(x) P(z|x) P(y|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3616e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_latent_variables() -> tuple[torch.Tensor, dict[str, int], list[str], list[tuple[str, str, str]]]:\n",
    "    \"\"\"\n",
    "    Demonstrate latent variables where z represents middle words between first (x) and last (y) words.\n",
    "    Uses the bigram model from section 1 and sampling from section 2.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate sequences using the bigram model from sections 1-2\n",
    "    num_samples = 1000\n",
    "    sequences = sample_from_bigram_model(conditional_probs, word_to_idx, num_sequences=num_samples, max_length=10)\n",
    "\n",
    "    # Convert sequences from indices to words and parse into x, z, y structure\n",
    "\n",
    "    parsed_examples = []\n",
    "    xy_counts = {}  # Count occurrences of (x, y) pairs\n",
    "    xyz_counts = {}  # Count occurrences of (x, z, y) triples\n",
    "\n",
    "    for i, seq_words in enumerate(sequences):\n",
    "        # sequences now contain words directly\n",
    "\n",
    "        # Skip sequences that are too short (need at least <s>, x, y, </s>)\n",
    "        if len(seq_words) < 4:\n",
    "            continue\n",
    "\n",
    "        # Parse into x, z, y\n",
    "        x = seq_words[1]  # First word after <s>\n",
    "        y = seq_words[-2]  # Last word before </s>\n",
    "        z_words = seq_words[2:-2]  # Middle words\n",
    "        z = \" \".join(z_words) if z_words else \"(empty)\"\n",
    "\n",
    "        # Count for empirical probability estimation\n",
    "        xy_pair = (x, y)\n",
    "        xyz_triple = (x, z, y)\n",
    "\n",
    "        xy_counts[xy_pair] = xy_counts.get(xy_pair, 0) + 1\n",
    "        xyz_counts[xyz_triple] = xyz_counts.get(xyz_triple, 0) + 1\n",
    "\n",
    "        # Show first few examples\n",
    "        if i < 8:\n",
    "            parsed_examples.append((x, z, y))\n",
    "\n",
    "    # Get unique first and last words for creating probability matrix\n",
    "    all_x_words = set()\n",
    "    all_y_words = set()\n",
    "    for x, y in xy_counts:\n",
    "        all_x_words.add(x)\n",
    "        all_y_words.add(y)\n",
    "\n",
    "    # Create sorted lists for consistent indexing\n",
    "    x_words = sorted(all_x_words)\n",
    "    y_words = sorted(all_y_words)\n",
    "\n",
    "    # Create empirical marginal probability matrix P(x, y)\n",
    "    # This is computed by marginalizing over all possible z values\n",
    "    marginal_xy = torch.zeros(len(x_words), len(y_words))\n",
    "    total_sequences = sum(xy_counts.values())\n",
    "\n",
    "    for (x, y), count in xy_counts.items():\n",
    "        if x in x_words and y in y_words:\n",
    "            x_idx = x_words.index(x)\n",
    "            y_idx = y_words.index(y)\n",
    "            marginal_xy[x_idx, y_idx] = count / total_sequences\n",
    "\n",
    "    # Show marginalization example for a common (x, y) pair\n",
    "    example_xy = None\n",
    "    max_count = 0\n",
    "    for xy_pair, count in xy_counts.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            example_xy = xy_pair\n",
    "\n",
    "    if example_xy:\n",
    "        x_ex, y_ex = example_xy\n",
    "        z_contributions = []\n",
    "        total_prob = 0\n",
    "        for (x, z, y), count in xyz_counts.items():\n",
    "            if x == x_ex and y == y_ex:\n",
    "                prob = count / total_sequences\n",
    "                total_prob += prob\n",
    "                z_contributions.append((z, prob))\n",
    "\n",
    "    # Create word-to-index mapping for visualization\n",
    "    word_to_idx_mapping = {word: idx for idx, word in enumerate(x_words + y_words)}\n",
    "\n",
    "    return (marginal_xy, word_to_idx_mapping, x_words + y_words, parsed_examples)\n",
    "\n",
    "\n",
    "# Run the latent variable demonstration\n",
    "marginal_xy, word_to_idx_mapping, all_words, parsed_examples = demonstrate_latent_variables()\n",
    "\n",
    "# Visualize the latent variable model\n",
    "fig, axes = create_subplot_grid(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Extract first and last words for visualization (limit to most common ones)\n",
    "x_words = sorted({x for x, _z, _y in parsed_examples})[:10]  # Top 10 first words\n",
    "y_words = sorted({y for _x, _z, y in parsed_examples})[:10]  # Top 10 last words\n",
    "\n",
    "# Create a subset of the marginal matrix for visualization\n",
    "marginal_subset = torch.zeros(len(x_words), len(y_words))\n",
    "for i, x_word in enumerate(x_words):\n",
    "    for j, y_word in enumerate(y_words):\n",
    "        if x_word in all_words and y_word in all_words:\n",
    "            x_idx = all_words.index(x_word) if x_word in all_words else -1\n",
    "            y_idx = all_words.index(y_word) if y_word in all_words else -1\n",
    "            if x_idx >= 0 and y_idx >= 0 and x_idx < marginal_xy.shape[0] and y_idx < marginal_xy.shape[1]:\n",
    "                marginal_subset[i, j] = marginal_xy[x_idx, y_idx]\n",
    "\n",
    "# Plot 1: Empirical first word frequencies\n",
    "x_counts = {}\n",
    "for x, _z, _y in parsed_examples:\n",
    "    x_counts[x] = x_counts.get(x, 0) + 1\n",
    "\n",
    "top_x_words = sorted(x_counts.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "x_words_plot = [word for word, _count in top_x_words]\n",
    "x_probs_plot = torch.tensor([count / len(parsed_examples) for _word, count in top_x_words])\n",
    "\n",
    "plot_bar_chart(\n",
    "    x_probs_plot.numpy(),\n",
    "    title=\"Empirical P(x) - First Word Frequencies\",\n",
    "    xlabel=\"First Word\",\n",
    "    ax=axes[0],\n",
    "    xticks=range(len(x_words_plot)),\n",
    "    xticklabels=x_words_plot,\n",
    ")\n",
    "\n",
    "# Plot 2: Sample of marginal probabilities P(x,y)\n",
    "plot_heatmap(\n",
    "    marginal_subset.numpy(),\n",
    "    \"Sample Marginal P(x,y) - First and Last Words\",\n",
    "    xlabel=\"Last Word (y)\",\n",
    "    ylabel=\"First Word (x)\",\n",
    "    cmap=\"Blues\",\n",
    "    xticks=range(len(y_words)),\n",
    "    yticks=range(len(x_words)),\n",
    "    xticklabels=y_words,\n",
    "    yticklabels=x_words,\n",
    "    ax=axes[1],\n",
    ")\n",
    "\n",
    "# Plot 3: Show z-word distribution for most common x,y pair\n",
    "z_counts = {}\n",
    "for _x, z, _y in parsed_examples:\n",
    "    z_counts[z] = z_counts.get(z, 0) + 1\n",
    "\n",
    "top_z_words = sorted(z_counts.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "z_words_plot = [z for z, _count in top_z_words]\n",
    "z_probs_plot = torch.tensor([count / len(parsed_examples) for _z, count in top_z_words])\n",
    "\n",
    "plot_bar_chart(\n",
    "    z_probs_plot.numpy(),\n",
    "    title=\"Distribution of Middle Words (z)\",\n",
    "    xlabel=\"Middle Word Sequence\",\n",
    "    ylabel=\"Frequency\",\n",
    "    ax=axes[2],\n",
    "    xticks=range(len(z_words_plot)),\n",
    "    xticklabels=z_words_plot,\n",
    ")\n",
    "\n",
    "# Plot 4: Show example sequences as text\n",
    "ax = axes[3]\n",
    "ax.axis(\"off\")\n",
    "ax.text(\n",
    "    0.1,\n",
    "    0.9,\n",
    "    \"Example Parsed Sequences:\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "y_pos = 0.8\n",
    "for i, (x, z, y) in enumerate(parsed_examples[:6]):  # Show first 6 examples\n",
    "    ax.text(\n",
    "        0.1,\n",
    "        y_pos,\n",
    "        f\"{i + 1}. x='{x}', z='{z}', y='{y}'\",\n",
    "        fontsize=10,\n",
    "        transform=ax.transAxes,\n",
    "        fontfamily=\"monospace\",\n",
    "    )\n",
    "    y_pos -= 0.12\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907125a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 6. Temperature Sampling and Biased Estimates\n",
    "\n",
    "When we don't sample from the true distribution, we get biased estimates. Temperature sampling modifies probabilities: $$P_T(x) = \\frac{\\exp(\\log P(x) / T)}{\\sum_i \\exp(\\log P(i) / T)}$$\n",
    "\n",
    "Let's demonstrate this by comparing bigram probability convergence at temperature T=1.0 (unbiased) vs T=0.5 (biased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_temperature_bias() -> tuple[list[int], list[float], list[float], list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Demonstrate how temperature sampling creates biased estimates by comparing\n",
    "    convergence at T=1.0 vs T=0.5 across the entire conditional distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample sizes to test\n",
    "    sample_sizes = [50, 100, 500, 1000, 5000, 10000]\n",
    "\n",
    "    # Results storage\n",
    "    mean_errors_t1 = []  # Temperature 1.0 (unbiased) - mean error across all bigrams\n",
    "    max_errors_t1 = []  # Temperature 1.0 (unbiased) - max error across all bigrams\n",
    "    mean_errors_t05 = []  # Temperature 0.5 (biased) - mean error across all bigrams\n",
    "    max_errors_t05 = []  # Temperature 0.5 (biased) - max error across all bigrams\n",
    "\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "    for n_samples in sample_sizes:\n",
    "        # Temperature 1.0 (unbiased sampling)\n",
    "        sequences_t1 = sample_from_bigram_model(\n",
    "            conditional_probs,\n",
    "            word_to_idx,\n",
    "            num_sequences=n_samples,\n",
    "            max_length=8,\n",
    "            temperature=1.0,\n",
    "        )\n",
    "        # Convert string sequences to index sequences for estimation\n",
    "        sequences_t1_idx = [[word_to_idx[word] for word in seq] for seq in sequences_t1]\n",
    "        _, estimated_conditional_t1 = estimate_conditional_probabilities_from_sequences(sequences_t1_idx, vocab_size)\n",
    "\n",
    "        # Calculate errors for all bigrams with non-zero true probability\n",
    "        errors_t1 = []\n",
    "        for i in range(vocab_size):\n",
    "            for j in range(vocab_size):\n",
    "                true_prob = conditional_probs[i, j].item()\n",
    "                if true_prob > 0:  # Only consider bigrams that actually exist\n",
    "                    estimated_prob = estimated_conditional_t1[i, j].item()\n",
    "                    error = abs(estimated_prob - true_prob)\n",
    "                    errors_t1.append(error)\n",
    "\n",
    "        mean_error_t1 = sum(errors_t1) / len(errors_t1)\n",
    "        max_error_t1 = max(errors_t1)\n",
    "        mean_errors_t1.append(mean_error_t1)\n",
    "        max_errors_t1.append(max_error_t1)\n",
    "\n",
    "        # Temperature 0.5 (biased sampling - more peaked)\n",
    "        sequences_t05 = sample_from_bigram_model(\n",
    "            conditional_probs,\n",
    "            word_to_idx,\n",
    "            num_sequences=n_samples,\n",
    "            max_length=8,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        # Convert string sequences to index sequences for estimation\n",
    "        sequences_t05_idx = [[word_to_idx[word] for word in seq] for seq in sequences_t05]\n",
    "        _, estimated_conditional_t05 = estimate_conditional_probabilities_from_sequences(sequences_t05_idx, vocab_size)\n",
    "\n",
    "        # Calculate errors for all bigrams with non-zero true probability\n",
    "        errors_t05 = []\n",
    "        for i in range(vocab_size):\n",
    "            for j in range(vocab_size):\n",
    "                true_prob = conditional_probs[i, j].item()\n",
    "                if true_prob > 0:  # Only consider bigrams that actually exist\n",
    "                    estimated_prob = estimated_conditional_t05[i, j].item()\n",
    "                    error = abs(estimated_prob - true_prob)\n",
    "                    errors_t05.append(error)\n",
    "\n",
    "        mean_error_t05 = sum(errors_t05) / len(errors_t05)\n",
    "        max_error_t05 = max(errors_t05)\n",
    "        mean_errors_t05.append(mean_error_t05)\n",
    "        max_errors_t05.append(max_error_t05)\n",
    "\n",
    "    return sample_sizes, mean_errors_t1, max_errors_t1, mean_errors_t05, max_errors_t05\n",
    "\n",
    "\n",
    "# Run the temperature bias demonstration\n",
    "sample_sizes, mean_errors_t1, max_errors_t1, mean_errors_t05, max_errors_t05 = demonstrate_temperature_bias()\n",
    "\n",
    "# Visualize the comparison - focus on mean error only\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot mean errors\n",
    "ax.loglog(\n",
    "    sample_sizes,\n",
    "    mean_errors_t1,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=\"T=1.0 (Unbiased)\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax.loglog(\n",
    "    sample_sizes,\n",
    "    mean_errors_t05,\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=\"T=0.5 (Biased)\",\n",
    "    marker=\"s\",\n",
    ")\n",
    "\n",
    "# Setup axis with utility function\n",
    "setup_axis_with_rotation(\n",
    "    ax,\n",
    "    title=\"Temperature Sampling Bias: Mean Error Across All Conditional Probabilities\",\n",
    "    xlabel=\"Number of Samples\",\n",
    "    ylabel=\"Mean Absolute Error\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ad0e53",
   "metadata": {},
   "source": [
    "# Meta-Generation: Cross-Model Likelihood Analysis\n",
    "\n",
    "By Graham Neubig for [11-664/763 Inference Algorithms for Language Modeling](https://phontron.com/class/lminference-fall2025/)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neubig/lminference-fall2025-code/blob/main/02-generation-basics/meta_generation.ipynb)\n",
    "\n",
    "This notebook explores meta-generation: the fascinating process of using one language model to generate text and then evaluating that text with different models. This approach reveals insights about model preferences, quality assessment, and the relationship between model size and text evaluation.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand cross-model evaluation and its applications\n",
    "- Implement log probability calculation for text sequences\n",
    "- Compare how different model sizes evaluate the same generated text\n",
    "- Analyze model agreement and disagreement patterns\n",
    "- Explore the relationship between generation and evaluation models\n",
    "\n",
    "## Key Concepts\n",
    "**Meta-Generation**: Using one model to generate text and another to evaluate it\n",
    "**Log Probability**: The logarithm of the probability a model assigns to a text sequence\n",
    "**Cross-Model Analysis**: Comparing how different models evaluate the same text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6626fb",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "First, let's import the necessary libraries and define our data structures. We'll use PyTorch for tensor operations, our custom GPT-2 implementation, and temperature sampling from the generation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf04b18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nanogpt import GPT2, GPT2Tokenizer\n",
    "from generation import temperature_sample\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GenerationResult:\n",
    "    \"\"\"Result of a single generation with cross-model evaluation\"\"\"\n",
    "\n",
    "    prompt: str\n",
    "    generated_text: str\n",
    "    small_log_prob: float\n",
    "    medium_log_prob: float\n",
    "    small_per_token_log_prob: float\n",
    "    medium_per_token_log_prob: float\n",
    "    token_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc191f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 1. Model Setup and Log Probability Calculation\n",
    "\n",
    "The foundation of meta-generation is the ability to calculate how likely a piece of text is according to different models. We'll set up two GPT-2 models of different sizes and implement log probability calculation.\n",
    "\n",
    "**Log Probability Calculation**: For a sequence of tokens $w_1, w_2, \\ldots, w_n$, the log probability is:\n",
    "$$\\log P(w_1, w_2, \\ldots, w_n) = \\sum_{i=1}^{n} \\log P(w_i | w_1, \\ldots, w_{i-1})$$\n",
    "\n",
    "We'll compare:\n",
    "- **Small Model**: GPT-2 base (117M parameters, 12 layers, 12 heads, 768 dimensions)\n",
    "- **Medium Model**: GPT-2 medium (345M parameters, 24 layers, 16 heads, 1024 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(device: str = \"auto\") -> tuple[GPT2, GPT2, GPT2Tokenizer]:\n",
    "    \"\"\"Load small and medium GPT-2 models for cross-model evaluation.\"\"\"\n",
    "    if device == \"auto\":\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        elif torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "    print(f\"Loading models on {device}...\")\n",
    "\n",
    "    # Create tokenizer\n",
    "    tokenizer = GPT2Tokenizer()\n",
    "\n",
    "    # Load small model (GPT-2 base model)\n",
    "    print(\"Loading GPT-2 small model...\")\n",
    "    small_model = GPT2.from_pretrained(\"gpt2\")\n",
    "    small_model.to(device)\n",
    "    small_model.eval()\n",
    "\n",
    "    # Load medium model (GPT-2 medium model)\n",
    "    print(\"Loading GPT-2 medium model...\")\n",
    "    medium_model = GPT2.from_pretrained(\"gpt2-medium\")\n",
    "    medium_model.to(device)\n",
    "    medium_model.eval()\n",
    "\n",
    "    print(\"Models loaded successfully!\")\n",
    "    return small_model, medium_model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_log_probability(\n",
    "    model: GPT2, tokenizer: GPT2Tokenizer, text: str, device: str\n",
    ") -> tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    Calculate log probability of text under a model.\n",
    "\n",
    "    Args:\n",
    "        model: GPT-2 model to evaluate with\n",
    "        tokenizer: Tokenizer for text processing\n",
    "        text: Text to evaluate\n",
    "        device: Device to run computation on\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (total_log_prob, per_token_log_prob, token_count)\n",
    "    \"\"\"\n",
    "    # Tokenize (handle special tokens like <|endoftext|>)\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    inputs = torch.tensor([tokens]).to(device)\n",
    "\n",
    "    if len(inputs[0]) <= 1:\n",
    "        return float(\"-inf\"), float(\"-inf\"), 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass targets to get logits for all positions\n",
    "        logits, _ = model(inputs, targets=inputs)\n",
    "\n",
    "        # Shift for next token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = inputs[..., 1:].contiguous()\n",
    "\n",
    "        # Calculate log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "        # Get log probabilities for actual tokens\n",
    "        token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Total and per-token log probability\n",
    "        total_log_prob = token_log_probs.sum().item()\n",
    "        per_token_log_prob = token_log_probs.mean().item()\n",
    "        token_count = len(shift_labels[0])\n",
    "\n",
    "    return total_log_prob, per_token_log_prob, token_count\n",
    "\n",
    "\n",
    "def demonstrate_log_probability_calculation() -> None:\n",
    "    \"\"\"Demonstrate log probability calculation with example texts.\"\"\"\n",
    "    print(\"Loading models for log probability demonstration...\")\n",
    "    small_model, medium_model, tokenizer = load_models()\n",
    "    device = next(small_model.parameters()).device\n",
    "\n",
    "    # Example texts with different expected qualities\n",
    "    example_texts = [\n",
    "        \"The future of artificial intelligence is bright and promising.\",\n",
    "        \"The future of artificial intelligence is banana purple elephant.\",\n",
    "        \"Artificial intelligence will transform many industries.\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nLog Probability Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, text in enumerate(example_texts, 1):\n",
    "        small_total, small_per_token, token_count = calculate_log_probability(small_model, tokenizer, text, str(device))\n",
    "        medium_total, medium_per_token, _ = calculate_log_probability(medium_model, tokenizer, text, str(device))\n",
    "\n",
    "        print(f\"\\n{i}. Text: '{text}'\")\n",
    "        print(f\"   Tokens: {token_count}\")\n",
    "        print(f\"   Small model  - Total: {small_total:.3f}, Per-token: {small_per_token:.3f}\")\n",
    "        print(f\"   Medium model - Total: {medium_total:.3f}, Per-token: {medium_per_token:.3f}\")\n",
    "\n",
    "        # Show which model prefers this text\n",
    "        if small_per_token > medium_per_token:\n",
    "            print(f\"   → Small model prefers this text (+{small_per_token - medium_per_token:.3f})\")\n",
    "        elif medium_per_token > small_per_token:\n",
    "            print(f\"   → Medium model prefers this text (+{medium_per_token - small_per_token:.3f})\")\n",
    "        else:\n",
    "            print(\"   → Models agree on this text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274ff07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run the log probability demonstration\n",
    "demonstrate_log_probability_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49766aac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Meta-Generation Experiment\n",
    "\n",
    "Now we'll run the core meta-generation experiment: generate text with one model and evaluate it with multiple models. This reveals how different models assess the same generated content.\n",
    "\n",
    "**Experimental Setup**:\n",
    "1. Generate text using the small model with temperature sampling\n",
    "2. Evaluate each generated text with both small and medium models\n",
    "3. Compare log probabilities to understand model preferences\n",
    "4. Analyze patterns in cross-model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_meta_generation_experiment(\n",
    "    prompt: str = \"The future of artificial intelligence is\",\n",
    "    num_generations: int = 100,\n",
    "    max_length: int = 30,\n",
    "    temperature: float = 1.0,\n",
    ") -> list[GenerationResult]:\n",
    "    \"\"\"\n",
    "    Run the meta-generation experiment: generate with small model, evaluate with both models.\n",
    "\n",
    "    Args:\n",
    "        prompt: Text prompt to start generation\n",
    "        num_generations: Number of texts to generate\n",
    "        max_length: Maximum tokens to generate per text\n",
    "        temperature: Sampling temperature for generation\n",
    "\n",
    "    Returns:\n",
    "        List of GenerationResult objects with cross-model evaluations\n",
    "    \"\"\"\n",
    "\n",
    "    # Load models\n",
    "    small_model, medium_model, tokenizer = load_models()\n",
    "    device = next(small_model.parameters()).device\n",
    "\n",
    "    print(f\"Generating {num_generations} outputs from small model...\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Temperature: {temperature}, Max length: {max_length}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_generations):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Progress: {i + 1}/{num_generations}\")\n",
    "\n",
    "        # Generate text using small model\n",
    "        device = next(small_model.parameters()).device\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        input_ids = torch.tensor([enc.encode(prompt, allowed_special={\"<|endoftext|>\"})]).to(device)\n",
    "        eos_token_id = enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        small_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                logits, _ = small_model(input_ids)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token = temperature_sample(next_token_logits, temperature)\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "                # Stop if we generate end-of-text token\n",
    "                if next_token.item() == eos_token_id:\n",
    "                    break\n",
    "\n",
    "        generated_text = tokenizer.decode(input_ids[0].tolist())\n",
    "\n",
    "        # Remove prompt from generated text\n",
    "        generated_only = generated_text[len(prompt) :].strip()\n",
    "\n",
    "        if not generated_only:\n",
    "            continue\n",
    "\n",
    "        # Calculate probabilities under both models\n",
    "        full_text = prompt + \" \" + generated_only\n",
    "\n",
    "        # Small model evaluation\n",
    "        small_total_log_prob, small_per_token_log_prob, token_count = calculate_log_probability(\n",
    "            small_model, tokenizer, full_text, str(device)\n",
    "        )\n",
    "\n",
    "        # Medium model evaluation\n",
    "        medium_total_log_prob, medium_per_token_log_prob, _ = calculate_log_probability(\n",
    "            medium_model, tokenizer, full_text, str(device)\n",
    "        )\n",
    "\n",
    "        result = GenerationResult(\n",
    "            prompt=prompt,\n",
    "            generated_text=generated_only,\n",
    "            small_log_prob=small_total_log_prob,\n",
    "            medium_log_prob=medium_total_log_prob,\n",
    "            small_per_token_log_prob=small_per_token_log_prob,\n",
    "            medium_per_token_log_prob=medium_per_token_log_prob,\n",
    "            token_count=token_count,\n",
    "        )\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    print(f\"\\nGenerated {len(results)} valid outputs\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def demonstrate_meta_generation() -> list[GenerationResult]:\n",
    "    \"\"\"Run a small-scale meta-generation demonstration.\"\"\"\n",
    "    print(\"Running meta-generation demonstration with 20 samples...\")\n",
    "\n",
    "    # Run a smaller experiment for demonstration\n",
    "    results = run_meta_generation_experiment(\n",
    "        prompt=\"The future of artificial intelligence is\", num_generations=20, max_length=25, temperature=1.0\n",
    "    )\n",
    "\n",
    "    # Show a few example results\n",
    "    print(\"\\nSample Results:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, result in enumerate(results[:5]):\n",
    "        print(f\"\\n{i+1}. Generated: '{result.generated_text[:60]}...'\")\n",
    "        print(f\"   Small model log prob: {result.small_per_token_log_prob:.3f}\")\n",
    "        print(f\"   Medium model log prob: {result.medium_per_token_log_prob:.3f}\")\n",
    "\n",
    "        if result.small_per_token_log_prob > result.medium_per_token_log_prob:\n",
    "            print(\"   → Small model prefers this text\")\n",
    "        elif result.medium_per_token_log_prob > result.small_per_token_log_prob:\n",
    "            print(\"   → Medium model prefers this text\")\n",
    "        else:\n",
    "            print(\"   → Models have similar preferences\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42f335",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run the meta-generation demonstration\n",
    "demo_results = demonstrate_meta_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005ab06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## 3. Cross-Model Analysis and Results\n",
    "\n",
    "After generating text with one model and evaluating with multiple models, we can analyze the results to understand model preferences and agreement patterns. This section provides comprehensive analysis tools.\n",
    "\n",
    "**Analysis Dimensions**:\n",
    "- **Combined Preferences**: Texts that both models rate highly\n",
    "- **Model Agreement**: How often models agree on text quality\n",
    "- **Disagreement Patterns**: Cases where models have opposing preferences\n",
    "- **Quality Indicators**: What makes text preferred by larger vs smaller models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_sort_results(results: list[GenerationResult]) -> dict[str, list[GenerationResult]]:\n",
    "    \"\"\"\n",
    "    Analyze and sort results by different criteria.\n",
    "\n",
    "    Args:\n",
    "        results: List of GenerationResult objects to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with sorted results by different criteria\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS: Sorting by Higher Log Probability (Both Models)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Sort by outputs that have higher log probability according to both models\n",
    "    sorted_by_both_total = sorted(results, key=lambda r: r.small_log_prob + r.medium_log_prob, reverse=True)\n",
    "\n",
    "    print(\"\\nTop 10 outputs with highest combined total log probability:\")\n",
    "    for i, result in enumerate(sorted_by_both_total[:10]):\n",
    "        print(f\"\\n{i+1}. Text: '{result.generated_text[:80]}...'\")\n",
    "        print(f\"   Small total log prob: {result.small_log_prob:.3f}\")\n",
    "        print(f\"   Medium total log prob: {result.medium_log_prob:.3f}\")\n",
    "        print(f\"   Combined: {result.small_log_prob + result.medium_log_prob:.3f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANALYSIS: Sorting by Higher Per-Token Log Probability (Both Models)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Sort by outputs that have higher per-token log probability according to both models\n",
    "    sorted_by_both_per_token = sorted(\n",
    "        results, key=lambda r: r.small_per_token_log_prob + r.medium_per_token_log_prob, reverse=True\n",
    "    )\n",
    "\n",
    "    print(\"\\nTop 10 outputs with highest combined per-token log probability:\")\n",
    "    for i, result in enumerate(sorted_by_both_per_token[:10]):\n",
    "        print(f\"\\n{i+1}. Text: '{result.generated_text[:80]}...'\")\n",
    "        print(f\"   Small per-token log prob: {result.small_per_token_log_prob:.3f}\")\n",
    "        print(f\"   Medium per-token log prob: {result.medium_per_token_log_prob:.3f}\")\n",
    "        print(f\"   Combined: {result.small_per_token_log_prob + result.medium_per_token_log_prob:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"combined_total\": sorted_by_both_total,\n",
    "        \"combined_per_token\": sorted_by_both_per_token,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_model_agreement(results: list[GenerationResult]) -> None:\n",
    "    \"\"\"Analyze patterns in model agreement and disagreement.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL AGREEMENT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Compare model preferences\n",
    "    small_better_count = sum(1 for r in results if r.small_per_token_log_prob > r.medium_per_token_log_prob)\n",
    "    medium_better_count = len(results) - small_better_count\n",
    "\n",
    "    print(f\"\\nModel preference comparison (per-token log probability):\")\n",
    "    print(\n",
    "        f\"Small model assigns higher probability: {small_better_count}/{len(results)} ({small_better_count/len(results)*100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Medium model assigns higher probability: {medium_better_count}/{len(results)} ({medium_better_count/len(results)*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # Calculate correlation between model scores\n",
    "    small_scores = [r.small_per_token_log_prob for r in results]\n",
    "    medium_scores = [r.medium_per_token_log_prob for r in results]\n",
    "\n",
    "    # Simple correlation calculation\n",
    "    mean_small = sum(small_scores) / len(small_scores)\n",
    "    mean_medium = sum(medium_scores) / len(medium_scores)\n",
    "\n",
    "    numerator = sum((s - mean_small) * (m - mean_medium) for s, m in zip(small_scores, medium_scores))\n",
    "    denom_small = sum((s - mean_small) ** 2 for s in small_scores) ** 0.5\n",
    "    denom_medium = sum((m - mean_medium) ** 2 for m in medium_scores) ** 0.5\n",
    "\n",
    "    correlation = numerator / (denom_small * denom_medium) if denom_small * denom_medium > 0 else 0\n",
    "    print(f\"\\nCorrelation between model scores: {correlation:.3f}\")\n",
    "\n",
    "    # Show examples where models disagree most\n",
    "    disagreement_scores = [(abs(r.small_per_token_log_prob - r.medium_per_token_log_prob), r) for r in results]\n",
    "    disagreement_scores.sort(reverse=True)\n",
    "\n",
    "    print(f\"\\nTop 3 examples where models disagree most:\")\n",
    "    for i, (disagreement, result) in enumerate(disagreement_scores[:3]):\n",
    "        preferred_model = \"Small\" if result.small_per_token_log_prob > result.medium_per_token_log_prob else \"Medium\"\n",
    "        print(f\"\\n{i+1}. Text: '{result.generated_text[:60]}...'\")\n",
    "        print(f\"   Small: {result.small_per_token_log_prob:.3f}, Medium: {result.medium_per_token_log_prob:.3f}\")\n",
    "        print(f\"   Disagreement: {disagreement:.3f} (prefers: {preferred_model})\")\n",
    "\n",
    "\n",
    "def run_comprehensive_analysis(results: list[GenerationResult]) -> None:\n",
    "    \"\"\"Run comprehensive analysis on meta-generation results.\"\"\"\n",
    "\n",
    "    # Sort and analyze results\n",
    "    sorted_results = analyze_and_sort_results(results)\n",
    "\n",
    "    # Analyze model agreement patterns\n",
    "    analyze_model_agreement(results)\n",
    "\n",
    "    # Additional insights\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY INSIGHTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    avg_small_score = sum(r.small_per_token_log_prob for r in results) / len(results)\n",
    "    avg_medium_score = sum(r.medium_per_token_log_prob for r in results) / len(results)\n",
    "\n",
    "    print(f\"\\nAverage per-token log probabilities:\")\n",
    "    print(f\"Small model: {avg_small_score:.3f}\")\n",
    "    print(f\"Medium model: {avg_medium_score:.3f}\")\n",
    "\n",
    "    if avg_medium_score > avg_small_score:\n",
    "        print(f\"→ Medium model generally assigns higher probabilities (+{avg_medium_score - avg_small_score:.3f})\")\n",
    "    else:\n",
    "        print(f\"→ Small model generally assigns higher probabilities (+{avg_small_score - avg_medium_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc127e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive analysis on demo results\n",
    "if \"demo_results\" in locals():\n",
    "    run_comprehensive_analysis(demo_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add3783e",
   "metadata": {},
   "source": [
    "# Meta-Generation Experiments: Cross-Model Likelihood Analysis\n",
    "\n",
    "By Graham Neubig for [11-664/763 Inference Algorithms for Language Modeling](https://phontron.com/class/lminference-fall2025/)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neubig/lminference-fall2025-code/blob/main/02-generation-basics/meta_generation.ipynb)\n",
    "\n",
    "This notebook implements fascinating meta-generation experiments that explore how different\n",
    "language models evaluate text generated by other models. We'll investigate the complex\n",
    "relationships between generators and evaluators in the language modeling space.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "Our experiments address several intriguing questions:\n",
    "\n",
    "1. **Cross-Model Evaluation**: How does GPT-2 Small vs Medium evaluate the same generated text?\n",
    "2. **Self-Preference Bias**: Do models prefer text they generated themselves?\n",
    "3. **Model Capability Patterns**: What patterns emerge in cross-model preferences?\n",
    "4. **Quality Assessment**: Can we use cross-model agreement as a quality signal?\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Understanding cross-model evaluation helps us:\n",
    "- Identify model biases and preferences\n",
    "- Explore the relationship between model size and text evaluation\n",
    "- Develop better quality assessment methods using multiple model perspectives\n",
    "- Gain meta-learning insights from model disagreements\n",
    "\n",
    "These experiments reveal deep insights about how language models \"think\" about text quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Install required packages\n",
    "%pip install torch>=2.5.0 numpy>=2.0.0 matplotlib>=3.9.0 seaborn>=0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f6072",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b93962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nanogpt import GPT2Tokenizer\n",
    "from nanogpt import (\n",
    "    GPT2,\n",
    "    GPT2Config,\n",
    ")\n",
    "from plotting_utils import plot_meta_generation_dashboard, save_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f86315",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MetaGenerationResult:\n",
    "    \"\"\"Result of meta-generation experiment\"\"\"\n",
    "\n",
    "    prompt: str\n",
    "    generator_model: str\n",
    "    generated_text: str\n",
    "    small_likelihood: float\n",
    "    medium_likelihood: float\n",
    "    likelihood_ratio: float  # medium - small (in log space)\n",
    "    perplexity_small: float\n",
    "    perplexity_medium: float\n",
    "    generation_time: float\n",
    "    text_length: int\n",
    "    cross_entropy_small: float\n",
    "    cross_entropy_medium: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7abf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrossModelComparison:\n",
    "    \"\"\"Comparison between models on the same text\"\"\"\n",
    "\n",
    "    text: str\n",
    "    prompt: str\n",
    "    generator: str\n",
    "    evaluations: dict[\n",
    "        str, dict[str, float]\n",
    "    ]  # model_name -> {likelihood, perplexity, etc.}\n",
    "    preference_winner: str\n",
    "    preference_strength: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikelihoodAnalyzer:\n",
    "    \"\"\"Analyzes text likelihood under different models\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: dict[str, GPT2],\n",
    "        tokenizers: dict[str, GPT2Tokenizer],\n",
    "        device: str,\n",
    "    ) -> None:\n",
    "        self.models = models\n",
    "        self.tokenizers = tokenizers\n",
    "        self.device = device\n",
    "\n",
    "    def compute_likelihood_metrics(\n",
    "        self, model_name: str, text: str\n",
    "    ) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute comprehensive likelihood metrics for text under a model\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with likelihood, perplexity, cross_entropy, and token-level stats\n",
    "        \"\"\"\n",
    "        model = self.models[model_name]\n",
    "        tokenizer = self.tokenizers[model_name]\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = torch.tensor([tokenizer.encode(text)]).to(self.device)\n",
    "\n",
    "        if len(inputs[0]) <= 1:\n",
    "            return {\n",
    "                \"likelihood\": float(\"-inf\"),\n",
    "                \"perplexity\": float(\"inf\"),\n",
    "                \"cross_entropy\": float(\"inf\"),\n",
    "                \"token_count\": 0,\n",
    "                \"avg_token_prob\": 0.0,\n",
    "            }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            loss = outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift for next token prediction\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs[..., 1:].contiguous()\n",
    "\n",
    "            # Calculate log probabilities\n",
    "            log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "            # Get log probabilities for actual tokens\n",
    "            token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(\n",
    "                -1\n",
    "            )\n",
    "\n",
    "            # Metrics\n",
    "            avg_log_likelihood = token_log_probs.mean().item()\n",
    "            perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "            cross_entropy = loss\n",
    "            token_count = len(shift_labels[0])\n",
    "            avg_token_prob = torch.exp(token_log_probs).mean().item()\n",
    "\n",
    "        return {\n",
    "            \"likelihood\": avg_log_likelihood,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"cross_entropy\": cross_entropy,\n",
    "            \"token_count\": token_count,\n",
    "            \"avg_token_prob\": avg_token_prob,\n",
    "        }\n",
    "\n",
    "    def compare_models_on_text(\n",
    "        self, text: str, prompt: str, generator: str\n",
    "    ) -> CrossModelComparison:\n",
    "        \"\"\"Compare how different models evaluate the same text\"\"\"\n",
    "        evaluations = {}\n",
    "\n",
    "        for model_name in self.models:\n",
    "            full_text = prompt + \" \" + text\n",
    "            metrics = self.compute_likelihood_metrics(model_name, full_text)\n",
    "            evaluations[model_name] = metrics\n",
    "\n",
    "        # Determine preference (higher likelihood wins)\n",
    "        model_names = list(evaluations.keys())\n",
    "        if len(model_names) >= 2:\n",
    "            model1, model2 = model_names[0], model_names[1]\n",
    "            likelihood_diff = (\n",
    "                evaluations[model2][\"likelihood\"] - evaluations[model1][\"likelihood\"]\n",
    "            )\n",
    "\n",
    "            if likelihood_diff > 0:\n",
    "                preference_winner = model2\n",
    "                preference_strength = likelihood_diff\n",
    "            else:\n",
    "                preference_winner = model1\n",
    "                preference_strength = abs(likelihood_diff)\n",
    "        else:\n",
    "            preference_winner = model_names[0] if model_names else \"none\"\n",
    "            preference_strength = 0.0\n",
    "\n",
    "        return CrossModelComparison(\n",
    "            text=text,\n",
    "            prompt=prompt,\n",
    "            generator=generator,\n",
    "            evaluations=evaluations,\n",
    "            preference_winner=preference_winner,\n",
    "            preference_strength=preference_strength,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaGenerationExperiment:\n",
    "    \"\"\"Comprehensive meta-generation experiments\"\"\"\n",
    "\n",
    "    def __init__(self, device: str = \"auto\") -> None:\n",
    "        \"\"\"Initialize the experiment\"\"\"\n",
    "        self.device = self._get_device(device)\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.results = []\n",
    "        self.analyzer = None\n",
    "\n",
    "        # Load models\n",
    "        self._load_models()\n",
    "\n",
    "        # Initialize analyzer\n",
    "        self.analyzer = LikelihoodAnalyzer(self.models, self.tokenizers, self.device)\n",
    "\n",
    "        # Test prompts - diverse to see different behaviors\n",
    "        self.prompts = [\n",
    "            \"The scientific method involves\",\n",
    "            \"In the depths of the ocean\",\n",
    "            \"Artificial intelligence will\",\n",
    "            \"The history of civilization shows\",\n",
    "            \"When I was a child\",\n",
    "            \"The economy is affected by\",\n",
    "            \"Space exploration has revealed\",\n",
    "            \"The human brain is\",\n",
    "            \"Climate change represents\",\n",
    "            \"Technology has transformed\",\n",
    "            \"The future of education\",\n",
    "            \"In quantum mechanics\",\n",
    "            \"The art of storytelling\",\n",
    "            \"Democracy requires\",\n",
    "            \"The universe contains\",\n",
    "        ]\n",
    "\n",
    "    def _get_device(self, device: str) -> str:\n",
    "        \"\"\"Determine the best device to use\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.backends.mps.is_available():\n",
    "                return \"mps\"\n",
    "            elif torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "\n",
    "    def _load_models(self) -> None:\n",
    "        \"\"\"Load GPT-2 models\"\"\"\n",
    "        print(\"🔄 Loading GPT-2 models for meta-generation...\")\n",
    "\n",
    "        # Create tokenizer\n",
    "        tokenizer = GPT2Tokenizer()\n",
    "\n",
    "        # Load different model configurations\n",
    "        configs = {\n",
    "            \"nanogpt-small\": GPT2Config(n_layer=6, n_head=6, n_embd=384),\n",
    "            \"nanogpt-medium\": GPT2Config(n_layer=12, n_head=12, n_embd=768),\n",
    "        }\n",
    "\n",
    "        for model_name, config in configs.items():\n",
    "            print(f\"  Creating {model_name}...\")\n",
    "\n",
    "            # Store tokenizer\n",
    "            self.tokenizers[model_name] = tokenizer\n",
    "\n",
    "            # Create model\n",
    "            model = GPT2(config)\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            self.models[model_name] = model\n",
    "\n",
    "            print(f\"    {model_name} created on {self.device}\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        prompt: str,\n",
    "        max_length: int = 50,\n",
    "        temperature: float = 0.8,\n",
    "        strategy: str = \"temperature\",\n",
    "    ) -> tuple[str, float]:\n",
    "        \"\"\"Generate text with specified model and strategy\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        tokenizer = self.tokenizers[model_name]\n",
    "\n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Prepare generation kwargs\n",
    "        gen_kwargs = {\n",
    "            \"input_ids\": inputs,\n",
    "            \"max_length\": len(inputs[0]) + max_length,\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            \"num_return_sequences\": 1,\n",
    "        }\n",
    "\n",
    "        if strategy == \"greedy\":\n",
    "            gen_kwargs[\"do_sample\"] = False\n",
    "        elif strategy == \"temperature\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "        elif strategy == \"top_p\":\n",
    "            gen_kwargs[\"do_sample\"] = True\n",
    "            gen_kwargs[\"top_p\"] = 0.9\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "        # Generate\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**gen_kwargs)\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt from the generated text\n",
    "        generated_only = generated_text[len(prompt) :].strip()\n",
    "\n",
    "        return generated_only, generation_time\n",
    "\n",
    "    def run_cross_model_evaluation(\n",
    "        self, max_length: int = 40\n",
    "    ) -> list[MetaGenerationResult]:\n",
    "        \"\"\"Run cross-model evaluation experiment\"\"\"\n",
    "        print(\"🔄 CROSS-MODEL EVALUATION EXPERIMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Comparing how GPT-2 small and medium evaluate generated text\\n\")\n",
    "\n",
    "        results = []\n",
    "        total_experiments = (\n",
    "            len(self.prompts) * len(self.models) * 2\n",
    "        )  # 2 strategies per model\n",
    "        experiment_count = 0\n",
    "\n",
    "        strategies = [\n",
    "            (\"temperature\", {\"temperature\": 0.8}),\n",
    "            (\"top_p\", {\"temperature\": 0.8}),\n",
    "        ]\n",
    "\n",
    "        for prompt in self.prompts:\n",
    "            print(f\"\\nPrompt: '{prompt}'\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Generate with each model using different strategies\n",
    "            for generator_model in self.models:\n",
    "                for strategy_name, strategy_kwargs in strategies:\n",
    "                    experiment_count += 1\n",
    "                    print(\n",
    "                        f\"[{experiment_count}/{total_experiments}] {generator_model} + {strategy_name}...\"\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        # Generate text\n",
    "                        generated_text, gen_time = self.generate_text(\n",
    "                            generator_model,\n",
    "                            prompt,\n",
    "                            max_length,\n",
    "                            strategy=strategy_name,\n",
    "                            **strategy_kwargs,\n",
    "                        )\n",
    "\n",
    "                        if not generated_text.strip():\n",
    "                            print(\"  Empty generation, skipping...\")\n",
    "                            continue\n",
    "\n",
    "                        full_text = prompt + \" \" + generated_text\n",
    "                        print(f\"  Generated: '{generated_text[:60]}...'\")\n",
    "\n",
    "                        # Compute likelihoods under both models\n",
    "                        print(\"  Computing cross-model likelihoods...\")\n",
    "                        if self.analyzer is None:\n",
    "                            print(\n",
    "                                \"  Analyzer not available, skipping likelihood computation...\"\n",
    "                            )\n",
    "                            continue\n",
    "                        small_metrics = self.analyzer.compute_likelihood_metrics(\n",
    "                            \"gpt2\", full_text\n",
    "                        )\n",
    "                        medium_metrics = self.analyzer.compute_likelihood_metrics(\n",
    "                            \"gpt2-medium\", full_text\n",
    "                        )\n",
    "\n",
    "                        # Calculate likelihood ratio (medium - small in log space)\n",
    "                        likelihood_ratio = (\n",
    "                            medium_metrics[\"likelihood\"] - small_metrics[\"likelihood\"]\n",
    "                        )\n",
    "\n",
    "                        result = MetaGenerationResult(\n",
    "                            prompt=prompt,\n",
    "                            generator_model=f\"{generator_model}_{strategy_name}\",\n",
    "                            generated_text=generated_text,\n",
    "                            small_likelihood=small_metrics[\"likelihood\"],\n",
    "                            medium_likelihood=medium_metrics[\"likelihood\"],\n",
    "                            likelihood_ratio=likelihood_ratio,\n",
    "                            perplexity_small=small_metrics[\"perplexity\"],\n",
    "                            perplexity_medium=medium_metrics[\"perplexity\"],\n",
    "                            generation_time=gen_time,\n",
    "                            text_length=len(generated_text.split()),\n",
    "                            cross_entropy_small=small_metrics[\"cross_entropy\"],\n",
    "                            cross_entropy_medium=medium_metrics[\"cross_entropy\"],\n",
    "                        )\n",
    "\n",
    "                        results.append(result)\n",
    "\n",
    "                        # Show likelihood comparison\n",
    "                        preference = \"Medium\" if likelihood_ratio > 0 else \"Small\"\n",
    "                        print(f\"  Small likelihood: {small_metrics['likelihood']:.3f}\")\n",
    "                        print(\n",
    "                            f\"  Medium likelihood: {medium_metrics['likelihood']:.3f}\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"  Ratio (M-S): {likelihood_ratio:.3f} → {preference} prefers this text\"\n",
    "                        )\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "        self.results = results\n",
    "        return results\n",
    "\n",
    "    def run_self_vs_other_evaluation(self, max_length: int = 30) -> dict[str, Any]:\n",
    "        \"\"\"Compare how models evaluate their own vs other models' generations\"\"\"\n",
    "        print(\"\\n🔄 SELF VS OTHER EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        self_preferences = {\"gpt2\": [], \"gpt2-medium\": []}\n",
    "        cross_preferences = {\"gpt2\": [], \"gpt2-medium\": []}\n",
    "\n",
    "        test_prompts = self.prompts[:5]  # Use subset for this analysis\n",
    "\n",
    "        for prompt in test_prompts:\n",
    "            print(f\"\\nAnalyzing: '{prompt[:40]}...'\")\n",
    "\n",
    "            # Generate with both models\n",
    "            generations = {}\n",
    "            for model_name in self.models:\n",
    "                text, _ = self.generate_text(model_name, prompt, max_length)\n",
    "                generations[model_name] = text\n",
    "\n",
    "            # Evaluate each generation under both models\n",
    "            if self.analyzer is None:\n",
    "                print(\"  Analyzer not available, skipping likelihood computation...\")\n",
    "                continue\n",
    "            for generator in generations:\n",
    "                for evaluator in self.models:\n",
    "                    full_text = prompt + \" \" + generations[generator]\n",
    "                    metrics = self.analyzer.compute_likelihood_metrics(\n",
    "                        evaluator, full_text\n",
    "                    )\n",
    "\n",
    "                    if generator == evaluator:\n",
    "                        # Self-evaluation\n",
    "                        self_preferences[evaluator].append(metrics[\"likelihood\"])\n",
    "                    else:\n",
    "                        # Cross-evaluation\n",
    "                        cross_preferences[evaluator].append(metrics[\"likelihood\"])\n",
    "\n",
    "        # Compute statistics\n",
    "        results = {}\n",
    "        for model in self.models:\n",
    "            if self_preferences[model] and cross_preferences[model]:\n",
    "                self_avg = np.mean(self_preferences[model])\n",
    "                cross_avg = np.mean(cross_preferences[model])\n",
    "                self_bias = self_avg - cross_avg\n",
    "\n",
    "                results[model] = {\n",
    "                    \"self_avg_likelihood\": self_avg,\n",
    "                    \"cross_avg_likelihood\": cross_avg,\n",
    "                    \"self_bias\": self_bias,\n",
    "                    \"prefers_own\": self_bias > 0,\n",
    "                }\n",
    "\n",
    "                print(f\"\\n{model} evaluation:\")\n",
    "                print(f\"  Own generations: {self_avg:.3f}\")\n",
    "                print(f\"  Other generations: {cross_avg:.3f}\")\n",
    "                print(\n",
    "                    f\"  Self-bias: {self_bias:.3f} ({'prefers own' if self_bias > 0 else 'prefers other'})\"\n",
    "                )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self) -> dict[str, Any]:\n",
    "        \"\"\"Comprehensive analysis of meta-generation results\"\"\"\n",
    "        print(\"\\nMETA-GENERATION ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if not self.results:\n",
    "            print(\"No results to analyze\")\n",
    "            return {}\n",
    "\n",
    "        # Basic statistics\n",
    "        total_results = len(self.results)\n",
    "        medium_preferred = sum(1 for r in self.results if r.likelihood_ratio > 0)\n",
    "        small_preferred = total_results - medium_preferred\n",
    "\n",
    "        print(f\"Total comparisons: {total_results}\")\n",
    "        print(\n",
    "            f\"Medium model prefers: {medium_preferred} ({medium_preferred / total_results * 100:.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Small model prefers: {small_preferred} ({small_preferred / total_results * 100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # Analyze by generator\n",
    "        print(\"\\nBy generator model:\")\n",
    "        generator_stats = {}\n",
    "        for result in self.results:\n",
    "            gen_model = result.generator_model\n",
    "            if gen_model not in generator_stats:\n",
    "                generator_stats[gen_model] = {\"total\": 0, \"medium_preferred\": 0}\n",
    "\n",
    "            generator_stats[gen_model][\"total\"] += 1\n",
    "            if result.likelihood_ratio > 0:\n",
    "                generator_stats[gen_model][\"medium_preferred\"] += 1\n",
    "\n",
    "        for gen_model, stats in generator_stats.items():\n",
    "            pct = stats[\"medium_preferred\"] / stats[\"total\"] * 100\n",
    "            print(\n",
    "                f\"  {gen_model}: {stats['medium_preferred']}/{stats['total']} ({pct:.1f}%) preferred by medium\"\n",
    "            )\n",
    "\n",
    "        # Find extreme cases\n",
    "        print(\"\\nExtreme preferences:\")\n",
    "\n",
    "        # Strongest medium preference\n",
    "        max_medium = max(self.results, key=lambda r: r.likelihood_ratio)\n",
    "        print(\n",
    "            f\"  Strongest medium preference (ratio: {max_medium.likelihood_ratio:.3f}):\"\n",
    "        )\n",
    "        print(f\"    Generator: {max_medium.generator_model}\")\n",
    "        print(f\"    Text: '{max_medium.generated_text[:80]}...'\")\n",
    "\n",
    "        # Strongest small preference\n",
    "        min_small = min(self.results, key=lambda r: r.likelihood_ratio)\n",
    "        print(\n",
    "            f\"  Strongest small preference (ratio: {min_small.likelihood_ratio:.3f}):\"\n",
    "        )\n",
    "        print(f\"    Generator: {min_small.generator_model}\")\n",
    "        print(f\"    Text: '{min_small.generated_text[:80]}...'\")\n",
    "\n",
    "        # Analyze correlations\n",
    "        lengths = [r.text_length for r in self.results]\n",
    "        ratios = [r.likelihood_ratio for r in self.results]\n",
    "        gen_times = [r.generation_time for r in self.results]\n",
    "\n",
    "        length_correlation = (\n",
    "            np.corrcoef(lengths, ratios)[0, 1] if len(lengths) > 1 else 0\n",
    "        )\n",
    "        time_correlation = (\n",
    "            np.corrcoef(gen_times, ratios)[0, 1] if len(gen_times) > 1 else 0\n",
    "        )\n",
    "\n",
    "        print(\"\\nCorrelations:\")\n",
    "        print(f\"  Text length vs likelihood ratio: {length_correlation:.3f}\")\n",
    "        print(f\"  Generation time vs likelihood ratio: {time_correlation:.3f}\")\n",
    "\n",
    "        # Quality metrics comparison\n",
    "        small_perplexities = [r.perplexity_small for r in self.results]\n",
    "        medium_perplexities = [r.perplexity_medium for r in self.results]\n",
    "\n",
    "        print(\"\\nPerplexity comparison:\")\n",
    "        print(f\"  Small model avg perplexity: {np.mean(small_perplexities):.2f}\")\n",
    "        print(f\"  Medium model avg perplexity: {np.mean(medium_perplexities):.2f}\")\n",
    "        print(\n",
    "            f\"  Perplexity improvement: {np.mean(small_perplexities) - np.mean(medium_perplexities):.2f}\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"total_results\": total_results,\n",
    "            \"medium_preferred\": medium_preferred,\n",
    "            \"small_preferred\": small_preferred,\n",
    "            \"generator_stats\": generator_stats,\n",
    "            \"correlations\": {\"length\": length_correlation, \"time\": time_correlation},\n",
    "            \"perplexity_stats\": {\n",
    "                \"small_avg\": np.mean(small_perplexities),\n",
    "                \"medium_avg\": np.mean(medium_perplexities),\n",
    "                \"improvement\": np.mean(small_perplexities)\n",
    "                - np.mean(medium_perplexities),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def create_visualizations(self) -> None:\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nCreating visualizations...\")\n",
    "\n",
    "        # Prepare data for dashboard\n",
    "        ratios = [r.likelihood_ratio for r in self.results]\n",
    "        lengths = [r.text_length for r in self.results]\n",
    "        gen_times = [r.generation_time for r in self.results]\n",
    "        small_perp = [r.perplexity_small for r in self.results]\n",
    "        medium_perp = [r.perplexity_medium for r in self.results]\n",
    "\n",
    "        # Compute generator statistics\n",
    "        generator_stats = {}\n",
    "        generator_ratios = {}\n",
    "        for result in self.results:\n",
    "            gen_model = result.generator_model\n",
    "            if gen_model not in generator_stats:\n",
    "                generator_stats[gen_model] = {\"total\": 0, \"medium_preferred\": 0}\n",
    "                generator_ratios[gen_model] = []\n",
    "            generator_stats[gen_model][\"total\"] += 1\n",
    "            if result.likelihood_ratio > 0:\n",
    "                generator_stats[gen_model][\"medium_preferred\"] += 1\n",
    "            generator_ratios[gen_model].append(result.likelihood_ratio)\n",
    "\n",
    "        # Create preference matrix for heatmap\n",
    "        unique_generators = list({r.generator_model for r in self.results})\n",
    "        preference_matrix = np.zeros((2, len(unique_generators)))\n",
    "\n",
    "        for i, gen in enumerate(unique_generators):\n",
    "            gen_results = [r for r in self.results if r.generator_model == gen]\n",
    "            avg_small_pref = np.mean([r.small_likelihood for r in gen_results])\n",
    "            avg_medium_pref = np.mean([r.medium_likelihood for r in gen_results])\n",
    "            preference_matrix[0, i] = avg_small_pref\n",
    "            preference_matrix[1, i] = avg_medium_pref\n",
    "\n",
    "        # Prepare data structure for dashboard\n",
    "        results_data = {\n",
    "            \"likelihood_ratios\": ratios,\n",
    "            \"text_lengths\": lengths,\n",
    "            \"generation_times\": gen_times,\n",
    "            \"small_perplexity\": small_perp,\n",
    "            \"medium_perplexity\": medium_perp,\n",
    "            \"generator_stats\": generator_stats,\n",
    "            \"generator_ratios\": generator_ratios,\n",
    "            \"preference_matrix\": preference_matrix,\n",
    "            \"unique_generators\": unique_generators,\n",
    "        }\n",
    "\n",
    "        # Create dashboard using plotting utilities\n",
    "        fig = plot_meta_generation_dashboard(results_data, list(self.models.keys()))\n",
    "        save_figure(fig, \"meta_generation_experiments_results\")\n",
    "        print(\"Visualizations saved: meta_generation_experiments_results.png/pdf\")\n",
    "\n",
    "    def save_results(self, filename: str = \"meta_generation_results.json\") -> None:\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to save\")\n",
    "            return\n",
    "\n",
    "        # Convert results to dictionaries\n",
    "        results_dict = [asdict(result) for result in self.results]\n",
    "\n",
    "        # Compute summary statistics\n",
    "        analysis = self.analyze_results()\n",
    "\n",
    "        data = {\n",
    "            \"device\": self.device,\n",
    "            \"total_prompts\": len(self.prompts),\n",
    "            \"total_results\": len(self.results),\n",
    "            \"results\": results_dict,\n",
    "            \"analysis\": analysis,\n",
    "            \"experiment_info\": {\n",
    "                \"models_used\": list(self.models.keys()),\n",
    "                \"prompts_used\": len(self.prompts),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "\n",
    "        print(f\"💾 Results saved: {filename}\")\n",
    "\n",
    "    def run_full_experiment(self) -> tuple[list[MetaGenerationResult], dict[str, Any]]:\n",
    "        \"\"\"Run the complete meta-generation experiment\"\"\"\n",
    "        print(\"META-GENERATION EXPERIMENTS: COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Run cross-model evaluation\n",
    "        results = self.run_cross_model_evaluation(max_length=35)\n",
    "\n",
    "        # Run self vs other evaluation\n",
    "        self_vs_other = self.run_self_vs_other_evaluation(max_length=30)\n",
    "\n",
    "        # Analyze results\n",
    "        analysis = self.analyze_results()\n",
    "\n",
    "        # Create visualizations\n",
    "        self.create_visualizations()\n",
    "\n",
    "        # Save results\n",
    "        self.save_results()\n",
    "\n",
    "        # Print summary\n",
    "        self._print_summary(analysis, self_vs_other)\n",
    "\n",
    "        return results, {\"analysis\": analysis, \"self_vs_other\": self_vs_other}\n",
    "\n",
    "    def _print_summary(self, analysis: dict, self_vs_other: dict) -> None:\n",
    "        \"\"\"Print experiment summary\"\"\"\n",
    "        print(\"\\nEXPERIMENT SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(f\"Total cross-evaluations: {analysis.get('total_results', 0)}\")\n",
    "        print(\n",
    "            f\"Medium model preference rate: {analysis.get('medium_preferred', 0) / analysis.get('total_results', 1) * 100:.1f}%\"\n",
    "        )\n",
    "\n",
    "        if self_vs_other:\n",
    "            print(\"\\nSelf-bias analysis:\")\n",
    "            for model, stats in self_vs_other.items():\n",
    "                bias_direction = (\n",
    "                    \"prefers own\" if stats[\"prefers_own\"] else \"prefers other\"\n",
    "                )\n",
    "                print(f\"  {model}: {bias_direction} (bias: {stats['self_bias']:.3f})\")\n",
    "\n",
    "        print(\"\\nKey insights:\")\n",
    "        if analysis.get(\"correlations\", {}).get(\"length\", 0) > 0.1:\n",
    "            print(\"  • Longer texts tend to be preferred by medium model\")\n",
    "        elif analysis.get(\"correlations\", {}).get(\"length\", 0) < -0.1:\n",
    "            print(\"  • Shorter texts tend to be preferred by medium model\")\n",
    "        else:\n",
    "            print(\"  • Text length has little correlation with model preference\")\n",
    "\n",
    "        perp_improvement = analysis.get(\"perplexity_stats\", {}).get(\"improvement\", 0)\n",
    "        if perp_improvement > 0:\n",
    "            print(\n",
    "                f\"  • Medium model shows {perp_improvement:.2f} lower perplexity on average\"\n",
    "            )\n",
    "\n",
    "        print(\"\\nMeta-generation experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main function to run meta-generation experiments\"\"\"\n",
    "    # Initialize experiment\n",
    "    experiment = MetaGenerationExperiment(device=\"auto\")\n",
    "\n",
    "    # Run full experiment\n",
    "    results, _ = experiment.run_full_experiment()\n",
    "\n",
    "    # Show some interesting examples\n",
    "    print(\"\\nINTERESTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if results:\n",
    "        # Most controversial (biggest disagreement)\n",
    "        most_controversial = max(results, key=lambda r: abs(r.likelihood_ratio))\n",
    "        print(\n",
    "            f\"\\nMost controversial text (ratio: {most_controversial.likelihood_ratio:.3f}):\"\n",
    "        )\n",
    "        print(f\"Generator: {most_controversial.generator_model}\")\n",
    "        print(f\"Prompt: '{most_controversial.prompt}'\")\n",
    "        print(f\"Text: '{most_controversial.generated_text}'\")\n",
    "\n",
    "        # Highest agreement for medium\n",
    "        if any(r.likelihood_ratio > 0 for r in results):\n",
    "            medium_favorite = max(\n",
    "                [r for r in results if r.likelihood_ratio > 0],\n",
    "                key=lambda r: r.likelihood_ratio,\n",
    "            )\n",
    "            print(\n",
    "                f\"\\nMedium model's strongest preference (ratio: {medium_favorite.likelihood_ratio:.3f}):\"\n",
    "            )\n",
    "            print(f\"Generator: {medium_favorite.generator_model}\")\n",
    "            print(f\"Text: '{medium_favorite.generated_text[:100]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
